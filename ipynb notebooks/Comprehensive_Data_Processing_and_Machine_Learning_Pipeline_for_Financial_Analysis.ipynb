{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ_78Hw7ui51",
        "outputId": "ada4c320-ac26-497f-a03a-8982b244ec54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wrds\n",
        "!pip install --upgrade numpy scipy pyarrow pandas datasets transformers\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "Lay3I0wXlg7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU availability"
      ],
      "metadata": {
        "id": "FoE0o46BlWYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "hALlhXU8lFOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing all necessary libraries and modules"
      ],
      "metadata": {
        "id": "6D-w2f1MlHUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "import torch\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import timedelta\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "from datasets import load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure NLTK stopwords are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "0jO5bHfrZHyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to WRDS"
      ],
      "metadata": {
        "id": "7Xr7wFkxlOKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wrds\n",
        "conn = wrds.Connection()"
      ],
      "metadata": {
        "id": "A_QuAhsqlMhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRSP Data Retrieval and Analysis\n",
        "\n",
        "### 1. In-sample data\n",
        "Historical stock data from 2005–2015 is analyzed to identify patterns and calculate financial metrics, focusing on the top 25 Consumer Discretionary companies by market capitalization.\n",
        "\n",
        "### 2. Out-sample data\n",
        "Stock data from 2016–2023 is used to validate the models developed, ensuring they perform well on new, unseen data.\n"
      ],
      "metadata": {
        "id": "VkFvN6ismGym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the date ranges for the estimation and out-of-sample periods\n",
        "estimation_period_start = '2005-01-01'\n",
        "estimation_period_end = '2015-12-31'\n",
        "out_sample_period_start = '2016-01-01'\n",
        "out_sample_period_end = '2023-12-31'\n",
        "\n",
        "# Defining the SIC code range for the Consumer Discretionary Sector\n",
        "sic_code_start = '5500'\n",
        "sic_code_end = '6000'"
      ],
      "metadata": {
        "id": "SZpkVF83mI6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-sample Data Retrieval and Analysis\n",
        "\n",
        "The in-sample data consists of CRSP stock data from 2005 to 2015 for companies in the Consumer Discretionary sector. This data is retrieved and processed to identify the top 25 companies based on average market capitalization.\n",
        "\n",
        "### Key Steps:\n",
        "1. **Data Retrieval**: The CRSP dataset is queried for relevant stock data, including prices, volume, and returns, filtered by the Consumer Discretionary sector's SIC code.\n",
        "2. **Data Preparation**: Dates are converted to a datetime format, and market capitalization is calculated for each company.\n",
        "3. **Top 25 Companies**: The top 25 companies by average market capitalization are identified and filtered from the dataset.\n",
        "4. **Data Cleaning**: The dataset is further cleaned by removing rows with missing values and duplicates.\n",
        "5. **Data Analysis**: Summary statistics and checks for missing values are performed to ensure data quality."
      ],
      "metadata": {
        "id": "CPwt7JhtoBS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving CRSP data for the estimation period (In-sample)\n",
        "crsp_data_estimation = conn.raw_sql(f\"\"\"\n",
        "    SELECT\n",
        "        stock.cusip,\n",
        "        stock.permno,\n",
        "        stock.permco,\n",
        "        company.comnam,\n",
        "        company.ticker,\n",
        "        stock.hsiccd,\n",
        "        stock.date,\n",
        "        stock.prc,\n",
        "        stock.vol,\n",
        "        stock.cfacpr,\n",
        "        stock.shrout,\n",
        "        stock.ret,\n",
        "        stock.retx\n",
        "    FROM\n",
        "        crsp.dsf AS stock\n",
        "    JOIN\n",
        "        crsp.dsenames AS company\n",
        "    ON\n",
        "        stock.permno = company.permno\n",
        "    WHERE\n",
        "        company.siccd BETWEEN '{sic_code_start}' AND '{sic_code_end}'\n",
        "        AND stock.date BETWEEN '{estimation_period_start}' AND '{estimation_period_end}'\n",
        "\"\"\")\n",
        "\n",
        "# Converting the 'date' column to datetime format\n",
        "crsp_data_estimation['date'] = pd.to_datetime(crsp_data_estimation['date'])\n",
        "\n",
        "# Calculating market capitalization\n",
        "crsp_data_estimation['market_cap'] = crsp_data_estimation['prc'] * crsp_data_estimation['shrout']\n",
        "\n",
        "# Calculating the average market capitalization for each company\n",
        "company_avg_market_cap = crsp_data_estimation.groupby('permno').agg({'market_cap': 'mean', 'comnam': 'first'}).reset_index()\n",
        "\n",
        "# Selecting the top 25 companies based on average market capitalization\n",
        "top_25_companies = company_avg_market_cap.nlargest(25, 'market_cap')\n",
        "top_25_permnos = top_25_companies['permno'].tolist()\n",
        "\n",
        "# Filtering the original estimation dataset to include only the top 25 companies\n",
        "top_25_estimation_data = crsp_data_estimation[crsp_data_estimation['permno'].isin(top_25_permnos)]\n",
        "\n",
        "# Displaying the information about the filtered dataset\n",
        "top_25_estimation_data.info()\n",
        "\n",
        "# Dropping rows with missing values\n",
        "top_25_estimation_data_clean = top_25_estimation_data.dropna()\n",
        "\n",
        "# Removing duplicate rows based on all columns\n",
        "top_25_estimation_data_clean.drop_duplicates(inplace=True)\n",
        "\n",
        "# Displaying the information about the cleaned dataset\n",
        "top_25_estimation_data_clean.info()\n",
        "\n",
        "# Summary Statistics\n",
        "summary_statistics = top_25_estimation_data_clean.describe()\n",
        "print(\"Summary Statistics:\\n\", summary_statistics)\n",
        "\n",
        "# Checking for missing values\n",
        "missing_values = top_25_estimation_data_clean.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "MUQiZ6i_mc4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Out-sample Data Retrieval and Analysis\n",
        "\n",
        "The out-sample data, covering the period from 2016 to 2023, focuses on the same top 25 companies identified during the in-sample period. This dataset is essential for validating the models' predictive performance in a more recent and unseen data context.\n",
        "\n",
        "### Key Steps:\n",
        "1. **Data Retrieval**: The CRSP dataset is queried for the out-sample period, filtering for companies in the Consumer Discretionary sector.\n",
        "2. **Data Preparation**: Dates are converted to a datetime format, and market capitalization is calculated for each company.\n",
        "3. **Top 25 Companies**: The dataset is filtered to include only the top 25 companies identified during the in-sample period.\n",
        "4. **Data Cleaning**: Rows with missing values are removed, and duplicates are dropped to ensure data quality.\n",
        "5. **Data Analysis**: Summary statistics and checks for missing values are performed on the cleaned out-sample dataset to validate its readiness for analysis."
      ],
      "metadata": {
        "id": "Zw9EScqloXg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving CRSP data for the out-of-sample period\n",
        "crsp_out_sample_data = conn.raw_sql(f\"\"\"\n",
        "    SELECT\n",
        "        stock.cusip,\n",
        "        stock.permno,\n",
        "        stock.permco,\n",
        "        company.comnam,\n",
        "        company.ticker,\n",
        "        stock.hsiccd,\n",
        "        stock.date,\n",
        "        stock.prc,\n",
        "        stock.vol,\n",
        "        stock.cfacpr,\n",
        "        stock.shrout,\n",
        "        stock.ret,\n",
        "        stock.retx\n",
        "    FROM\n",
        "        crsp.dsf AS stock\n",
        "    JOIN\n",
        "        crsp.dsenames AS company\n",
        "    ON\n",
        "        stock.permno = company.permno\n",
        "    WHERE\n",
        "        company.siccd BETWEEN '{sic_code_start}' AND '{sic_code_end}'\n",
        "        AND stock.date BETWEEN '{out_sample_period_start}' AND '{out_sample_period_end}'\n",
        "\"\"\")\n",
        "\n",
        "# Converting the 'date' column to datetime format\n",
        "crsp_out_sample_data['date'] = pd.to_datetime(crsp_out_sample_data['date'])\n",
        "\n",
        "# Filtering the out-of-sample dataset to include only the top 25 companies identified in the estimation period\n",
        "out_sample_top_25_data = crsp_out_sample_data[crsp_out_sample_data['permno'].isin(top_25_permnos)]\n",
        "\n",
        "# Calculating market capitalization\n",
        "out_sample_top_25_data['market_cap'] = out_sample_top_25_data['prc'] * out_sample_top_25_data['shrout']\n",
        "\n",
        "# Dropping rows with missing values\n",
        "out_sample_top_25_data_clean = out_sample_top_25_data.dropna()\n",
        "\n",
        "# Removing duplicate rows based on all columns\n",
        "out_sample_top_25_data_clean.drop_duplicates(inplace=True)\n",
        "\n",
        "# Displaying the information about the cleaned out-of-sample dataset\n",
        "out_sample_top_25_data_clean.info()\n",
        "\n",
        "out_sample_summary_statistics = out_sample_top_25_data_clean.describe()\n",
        "print(\"Out-of-Sample Data Summary Statistics:\\n\", out_sample_summary_statistics)\n",
        "\n",
        "out_sample_missing_values = out_sample_top_25_data_clean.isnull().sum()\n",
        "print(\"Missing Values after Cleaning (Out-of-Sample Data):\\n\", out_sample_missing_values)"
      ],
      "metadata": {
        "id": "zLpflXrtmy9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sorting and Saving Cleaned Data\n",
        "\n",
        "After cleaning the data, it's essential to ensure that the dataset is organized chronologically. This section involves sorting the estimation and out-of-sample data by date to maintain the temporal sequence, which is critical for accurate analysis and forecasting. The sorted datasets are then saved for future use.\n",
        "\n",
        "### Key Steps:\n",
        "\n",
        "1. **Sorting Data by Date**:\n",
        "   - Both the cleaned estimation (in-sample) and out-of-sample datasets are sorted by the date column. This step ensures that all data points are in chronological order, which is crucial for time series analysis and modeling.\n",
        "\n",
        "2. **Saving Sorted Data**:\n",
        "   - The sorted datasets are saved as CSV files, making them easily accessible for further analysis and model training. This step is particularly important as it preserves the data in a structured format, ready for use in subsequent stages of the research."
      ],
      "metadata": {
        "id": "Ij2HZnbTpzyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting the cleaned in-sample data by date\n",
        "top_25_estimation_data_clean_sorted = top_25_estimation_data_clean.sort_values(by='date')\n",
        "\n",
        "# Sorting the cleaned out-sample data by date\n",
        "out_sample_top_25_data_clean_sorted = out_sample_top_25_data_clean.sort_values(by='date')\n",
        "\n",
        "# Saving the sorted in-sample data to a CSV file for future use\n",
        "top_25_estimation_data_clean_sorted.to_csv('/content/top_25_data_estimation_period.csv', index=False)\n",
        "\n",
        "# Saving the sorted out-sample data to a CSV file for future use\n",
        "out_sample_top_25_data_clean_sorted.to_csv('/content/top_25_data_outsample_period.csv', index=False)"
      ],
      "metadata": {
        "id": "9dyT--rMppxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Market Capitalization\n",
        "\n",
        "To gain a deeper understanding of the market capitalization trends for the top 25 companies, several visualizations were created. These plots provide insights into the distribution and changes in market capitalization over time, both during the in-sample and out-sample periods.\n",
        "\n",
        "### Key Visualizations:\n",
        "\n",
        "1. **Distribution of Market Capitalization**:\n",
        "   - A histogram was plotted to visualize the distribution of market capitalization across the top 25 companies.\n",
        "   - This helps identify how market capitalization is spread among these companies, highlighting any significant variations.\n",
        "\n",
        "2. **Average Market Capitalization Over Time**:\n",
        "   - Time series plots were generated to observe the average market capitalization over the estimation and out-sample periods.\n",
        "   - These plots show trends and fluctuations in market capitalization, providing a clear view of how the overall market has evolved over time.\n",
        "\n",
        "3. **Top 25 Companies by Market Capitalization**:\n",
        "   - A simple listing of the top 25 companies based on market capitalization during both the in-sample and out-sample periods.\n",
        "   - This allows for a direct comparison of which companies consistently maintained high market capitalization."
      ],
      "metadata": {
        "id": "TqBx3oJ0pTXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distribution of market capitalization for the top 25 companies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(top_25_estimation_data_clean['market_cap'], bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Market Capitalization for Top 25 Companies')\n",
        "plt.xlabel('Market Capitalization')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the average market capitalization over time for the in-sample period\n",
        "avg_market_cap_time_series = top_25_estimation_data_clean_sorted.groupby('date')['market_cap'].mean()\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(avg_market_cap_time_series, color='blue')\n",
        "plt.title('Average Market Capitalization Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Market Capitalization')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the average market capitalization over time for the out-sample period\n",
        "avg_market_cap_time_series = out_sample_top_25_data_clean_sorted.groupby('date')['market_cap'].mean()\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(avg_market_cap_time_series, color='blue')\n",
        "plt.title('Average Market Capitalization Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Market Capitalization')\n",
        "plt.show()\n",
        "\n",
        "# Displaying the top 25 companies by market capitalization during the in-sample period\n",
        "print(\"Top 25 Companies based on Market Capitalization during the In-Sample Period:\")\n",
        "top_25_estimation_data_clean_sorted['comnam'][:25]\n",
        "\n",
        "# Displaying the top 25 companies by market capitalization during the out-sample period\n",
        "print(\"\\nTop 25 Companies based on Market Capitalization during the Out-Sample Period:\")\n",
        "out_sample_top_25_data_clean_sorted['comnam'][:25]"
      ],
      "metadata": {
        "id": "SvvWe9uDntDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing available tables in the 'crsp' library\n",
        "crsp_tables = conn.list_tables(library='crsp')\n",
        "print(\"Available CRSP tables:\\n\", crsp_tables)"
      ],
      "metadata": {
        "id": "-thbXDoBqL3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting and Storing GV Key Data\n",
        "\n",
        "For linking financial data across different datasets, obtaining unique identifiers like GV Keys is essential. This section outlines the process of fetching, cleaning, and storing GV Key data, which is crucial for accurate merging of datasets.\n",
        "\n",
        "### Key Steps:\n",
        "\n",
        "1. **Fetching GV Key Data**:\n",
        "   - GV Key values are retrieved for the top 25 companies, identified by their `permnos`, from the CRSP database. GV Keys serve as unique identifiers, allowing for precise matching across different financial datasets.\n",
        "\n",
        "2. **Cleaning and Removing Duplicates**:\n",
        "   - The fetched GV Key data is checked for duplicates, and any redundant entries are removed. This ensures that each company is represented by a unique GV Key, preventing potential errors during data merging.\n",
        "\n",
        "3. **Saving Cleaned GV Key Data**:\n",
        "   - The cleaned GV Key data is saved as a CSV file for easy access in subsequent steps. This file will be used to merge the CRSP data with other datasets, ensuring accurate alignment based on GV Keys.\n",
        "\n",
        "By securing and cleaning the GV Key data, this step ensures that the integrity of the dataset is maintained when merging with other data sources. The stored GV Key file provides a reliable reference for future data operations."
      ],
      "metadata": {
        "id": "rfLTOVdVqgj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the gvkey values for the top 25 permnos from crsp.ccm_lookup\n",
        "gvkey_data_query = f\"\"\"\n",
        "    SELECT gvkey, lpermno, lpermco\n",
        "    FROM crsp.ccm_lookup\n",
        "    WHERE lpermno IN ({','.join(map(str, top_25_permnos))})\n",
        "\"\"\"\n",
        "gvkey_data = conn.raw_sql(gvkey_data_query)\n",
        "\n",
        "# Displaying the fetched GV Key data\n",
        "print(\"Fetched GV Key Data:\\n\", gvkey_data.head())\n",
        "\n",
        "# Removing duplicate rows from the GV Key data\n",
        "gvkey_data_unique = gvkey_data.drop_duplicates()\n",
        "\n",
        "# Displaying the unique GV Key data\n",
        "print(\"Unique GV Key Data:\\n\", gvkey_data_unique.head())\n",
        "\n",
        "gvkeys = gvkey_data_unique['gvkey'].tolist()\n",
        "\n",
        "# Saving the unique GV Key data to a CSV file for further use\n",
        "gvkey_data_unique.to_csv('/content/gvkey_data_unique.csv', index=False)"
      ],
      "metadata": {
        "id": "HRhuHYH7ppjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Key Developments Data from Capital IQ\n",
        "\n",
        "The process of retrieving Key Developments data from the Capital IQ database involves several critical steps, focusing on identifying, extracting, and preparing the data for analysis. This section details how key financial events are pulled from the database for both in-sample and out-of-sample periods.\n",
        "\n",
        "### Key Steps:\n",
        "\n",
        "1. **Identifying Available Tables**:\n",
        "   - The first step involves listing the available tables in the `ciq_keydev` library within the Capital IQ database. This ensures that the relevant data sources are identified before proceeding with data extraction.\n",
        "\n",
        "2. **Defining Date Ranges**:\n",
        "   - Specific date ranges are defined to categorize the data into in-sample (2005-2015) and out-of-sample (2016-2023) periods. These date ranges are crucial for segmenting the data for various analytical purposes.\n",
        "\n",
        "3. **Extracting Key Development Data**:\n",
        "   - Queries are executed to fetch key development data from the `ciq_keydev.wrds_keydev` table for both the estimation and out-of-sample periods. This data includes significant financial events, such as mergers, acquisitions, and earnings announcements, associated with the companies identified by their GV Keys.\n",
        "\n",
        "4. **Retrieving Additional Event Details**:\n",
        "   - A subsequent query is performed to retrieve specific details such as `keydevid`, `headline`, `situation`, and `announceddate` from the `ciq_keydev.ciqkeydev` table, based on unique `keydevids` obtained from the initial extraction.\n",
        "\n",
        "By methodically pulling and preparing this data, the dataset becomes enriched with detailed financial events, which can then be used to conduct in-depth analysis and model development."
      ],
      "metadata": {
        "id": "cUxjlPHzq-7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing available tables in the 'ciq_keydev' library\n",
        "ciq_keydev_tables = conn.list_tables(library='ciq_keydev')\n",
        "\n",
        "# Displaying the available Capital IQ Key Developments tables\n",
        "print(\"Available Capital IQ Key Developments Tables:\\n\", ciq_keydev_tables)\n",
        "\n",
        "# Defining the date ranges\n",
        "estimation_start_date = '2005-01-01'\n",
        "estimation_end_date = '2015-12-31'\n",
        "out_sample_start_date = '2016-01-01'\n",
        "end_date = '2023-12-31'\n",
        "\n",
        "# Fetching key development data for the estimation period from ciq_keydev.wrds_keydev (ciqkeydev)\n",
        "keydev_data_estimation_query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM ciq_keydev.wrds_keydev\n",
        "    WHERE gvkey IN ({','.join(\"'\" + str(gvkey) + \"'\" for gvkey in gvkeys)})\n",
        "    AND announcedate BETWEEN '{estimation_start_date}' AND '{estimation_end_date}'\n",
        "\"\"\"\n",
        "keydev_data_estimation = conn.raw_sql(keydev_data_estimation_query)\n",
        "\n",
        "# Fetching key development data for the out-of-sample period from ciq_keydev.wrds_keydev\n",
        "keydev_data_out_sample_query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM ciq_keydev.wrds_keydev\n",
        "    WHERE gvkey IN ({','.join(\"'\" + str(gvkey) + \"'\" for gvkey in gvkeys)})\n",
        "    AND announcedate BETWEEN '{out_sample_start_date}' AND '{end_date}'\n",
        "\"\"\"\n",
        "keydev_data_out_sample = conn.raw_sql(keydev_data_out_sample_query)\n",
        "\n",
        "unique_keydevids = keydev_data_estimation[\"keydevid\"].unique()\n",
        "\n",
        "query = f\"\"\"\n",
        "    SELECT keydevid, headline, situation, announceddate\n",
        "    FROM ciq_keydev.ciqkeydev\n",
        "    WHERE keydevid IN ({','.join([f\"'{str(k)}'\" for k in unique_keydevids])})\n",
        "\"\"\"\n",
        "\n",
        "data_sample = conn.raw_sql(query)\n",
        "data_sample.head()"
      ],
      "metadata": {
        "id": "zZdxsD0uqqHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging and Saving Key Development Data\n",
        "\n",
        "After extracting the relevant Key Developments data from Capital IQ, the next critical step is merging these datasets to create comprehensive data files that include both the situational context and headlines associated with each key development event.\n",
        "\n",
        "### Key Steps:\n",
        "\n",
        "1. **Merging Datasets**:\n",
        "   - The datasets retrieved for both the estimation and out-of-sample periods are merged with the supplementary data containing `situation` and `headline` details. This merge is based on the `keydevid` and `headline` fields, ensuring that each financial event is enriched with the corresponding descriptive information.\n",
        "\n",
        "2. **Reviewing the Merged Dataset**:\n",
        "   - After merging, a sample of the resulting dataset is displayed to verify that the merge operation was successful and that the data is correctly aligned.\n",
        "\n",
        "3. **Saving the Merged Data**:\n",
        "   - The final merged datasets for both the estimation and out-of-sample periods are saved as CSV files. These files serve as the basis for further analysis, providing a comprehensive view of the financial events and their associated details over the defined periods."
      ],
      "metadata": {
        "id": "PoMhUtlVrREx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the datasets based on 'keydevid'\n",
        "keydev_data_estimation_merged_dataset = pd.merge(keydev_data_estimation, data_sample[['keydevid', 'situation', 'headline']],\n",
        "                          on=['keydevid', 'headline'], how='left')\n",
        "\n",
        "keydev_data_out_sample_merged_dataset = pd.merge(keydev_data_out_sample, data_sample[['keydevid', 'situation', 'headline']],\n",
        "                          on=['keydevid', 'headline'], how='left')\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(\"Merged Dataset Head:\")\n",
        "keydev_data_estimation_merged_dataset.head()\n",
        "\n",
        "# Saving the fetched data to CSV files for further use\n",
        "keydev_data_estimation_merged_dataset.to_csv('/content/keydev_data_estimation.csv', index=False)\n",
        "keydev_data_out_sample_merged_dataset.to_csv('/content/keydev_data_out_sample.csv', index=False)"
      ],
      "metadata": {
        "id": "9a1aXvrmqp-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Processed Data for Analysis\n",
        "\n",
        "Before proceeding with the analysis, the previously saved datasets need to be loaded back into the environment. These datasets include the top 25 companies' data, GV Keys, and Key Developments data for both the estimation and out-of-sample periods.\n",
        "\n",
        "### Key Steps:\n",
        "\n",
        "1. **Loading Top 25 Companies' Data**:\n",
        "   - The data for the top 25 companies identified during the estimation and out-of-sample periods are loaded from their respective CSV files. These datasets contain detailed financial metrics that will be used for further analysis.\n",
        "\n",
        "2. **Loading GV Key Data**:\n",
        "   - The unique GV Key data, which maps the top 25 companies to their respective GV Keys, is also loaded. This data is crucial for linking financial data across different datasets.\n",
        "\n",
        "3. **Extracting GV Keys**:\n",
        "   - The GV Keys are extracted into a list to facilitate their use in further database queries or operations.\n",
        "\n",
        "4. **Loading Key Developments Data**:\n",
        "   - The Key Developments data, which details significant financial events for the top 25 companies, is loaded for both the estimation and out-of-sample periods. This data will be used to analyze the impact of these events on stock prices and market behavior."
      ],
      "metadata": {
        "id": "Ur-roSBHro4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the top 25 estimation data from the CSV file\n",
        "top_25_estimation_data = pd.read_csv('/content/top_25_data_estimation_period.csv')\n",
        "\n",
        "# Loading the top 25 out-of-sample data from the CSV file\n",
        "top_25_out_sample_data = pd.read_csv('/content/top_25_data_outsample_period.csv')\n",
        "\n",
        "# Loading the unique GV Key data from the CSV file\n",
        "gvkey_data_unique = pd.read_csv('/content/gvkey_data_unique.csv')\n",
        "\n",
        "# Extracting the GV Keys into a list\n",
        "gvkeys = gvkey_data_unique['gvkey'].tolist()\n",
        "\n",
        "# Displaying the list of GV Keys\n",
        "print(\"List of GV Keys:\\n\", gvkeys)\n",
        "\n",
        "# Loading the key development data for the estimation period from the CSV file\n",
        "keydev_data_estimation = pd.read_csv('/content/keydev_data_estimation.csv')\n",
        "\n",
        "# Loading the key development data for the out-of-sample period from the CSV file\n",
        "keydev_data_out_sample = pd.read_csv('/content/keydev_data_out_sample.csv')"
      ],
      "metadata": {
        "id": "CiYdABcyrZIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Key Development Data with GV Key Information\n",
        "\n",
        "To enhance the dataset for analysis, it's essential to merge the Key Development data with the GV Key data. This step helps in associating the financial events with specific companies by linking them through unique identifiers.\n",
        "\n",
        "### Key Steps:\n",
        "\n",
        "1. **Merging for the Estimation Period**:\n",
        "   - The Key Development data for the estimation period is merged with the GV Key data to include `lpermno` and `lpermco` identifiers. This merge ensures that each financial event is accurately linked to the corresponding company.\n",
        "\n",
        "2. **Merging for the Out-of-Sample Period**:\n",
        "   - Similarly, the Key Development data for the out-of-sample period is merged with the GV Key data. This merge is crucial for maintaining consistency in the analysis and ensuring that the same identifiers are used across both periods."
      ],
      "metadata": {
        "id": "ICMBJNS7sGZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging key development data with gvkey_data_unique to get lpermno and lpermco for estimation period\n",
        "merged_data_estimation = pd.merge(keydev_data_estimation, gvkey_data_unique, on='gvkey', how='left')\n",
        "\n",
        "# Merging key development data with gvkey_data_unique to get lpermno and lpermco for out-of-sample period\n",
        "merged_data_out_sample = pd.merge(keydev_data_out_sample, gvkey_data_unique, on='gvkey', how='left')\n",
        "\n",
        "print(merged_data_estimation.head(2))\n",
        "print(merged_data_out_sample.head(2))"
      ],
      "metadata": {
        "id": "PLYoH3GMrZse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refining the Key Development Data\n",
        "\n",
        "After merging the Key Development data with GV Key information, the next step is to refine the dataset by selecting the most relevant columns and ensuring consistency in the data formats.\n",
        "\n",
        "### Key Steps:\n",
        "\n",
        "1. **Column Selection**:\n",
        "   - From the merged datasets, only the most pertinent columns are retained. These include identifiers (`keydevid`, `companyid`, `companyname`), event details (`headline`, `situation`, `keydeveventtypeid`, `eventtype`, `announcedate`, `announcetime`, `mostimportantdateutc`), and company-specific identifiers (`gvkey`, `lpermco`, `lpermno`).\n",
        "\n",
        "2. **Date Conversion**:\n",
        "   - The `announcedate` column in both the estimation and out-of-sample datasets is converted to a datetime format to ensure accurate time-based analysis.\n",
        "\n",
        "3. **Column Renaming**:\n",
        "   - To maintain uniformity, the columns `lpermco` and `lpermno` are renamed to `permco` and `permno` respectively in both datasets.\n",
        "\n",
        "4. **Duplicate Removal**:\n",
        "   - Any duplicate rows within the estimation and out-of-sample datasets are removed, ensuring that the data is clean and ready for subsequent analysis."
      ],
      "metadata": {
        "id": "7rTniokcsYpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the selected columns to keep\n",
        "selected_columns = ['keydevid', 'companyid', 'companyname', 'headline', 'situation', 'keydeveventtypeid', 'eventtype', 'announcedate', 'announcetime', 'mostimportantdateutc', 'gvkey', 'lpermco', 'lpermno']\n",
        "\n",
        "# Select the relevant columns for the estimation period\n",
        "key_data_estimation = merged_data_estimation[selected_columns]\n",
        "\n",
        "# Select the relevant columns for the out-of-sample period\n",
        "key_data_out_sample = merged_data_out_sample[selected_columns]\n",
        "\n",
        "# Convert the 'announcedate' column to datetime format for both datasets\n",
        "key_data_estimation['announcedate'] = pd.to_datetime(key_data_estimation['announcedate'])\n",
        "key_data_out_sample['announcedate'] = pd.to_datetime(key_data_out_sample['announcedate'])\n",
        "\n",
        "# Rename columns 'lpermco' and 'lpermno' to 'permco' and 'permno' for the estimation period\n",
        "key_data_estimation.rename(columns={'lpermco': 'permco', 'lpermno': 'permno'}, inplace=True)\n",
        "\n",
        "# Rename columns 'lpermco' and 'lpermno' to 'permco' and 'permno' for the out-of-sample period\n",
        "key_data_out_sample.rename(columns={'lpermco': 'permco', 'lpermno': 'permno'}, inplace=True)\n",
        "\n",
        "# Drop duplicate rows in the estimation dataset\n",
        "key_data_estimation.drop_duplicates(inplace=True)\n",
        "\n",
        "# Drop duplicate rows in the out-of-sample dataset\n",
        "key_data_out_sample.drop_duplicates(inplace=True)\n",
        "\n",
        "# Display information about the cleaned estimation dataset\n",
        "print(key_data_estimation.info())\n",
        "\n",
        "# Display information about the cleaned out-of-sample dataset\n",
        "print(key_data_out_sample.info())"
      ],
      "metadata": {
        "id": "GyXMSNaxrzle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Event Type Analysis in Key Developments\n",
        "\n",
        "This section highlights the most common financial events within the Key Developments dataset.\n",
        "\n",
        "### Key Steps:\n",
        "\n",
        "1. **Top 20 Event Types**:\n",
        "   - The most frequent event types are identified and selected for analysis.\n",
        "\n",
        "2. **Visualization**:\n",
        "   - A bar plot visualizes the distribution of these top 20 event types, using a clear and engaging color scheme."
      ],
      "metadata": {
        "id": "S5VlCl_PtLlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the top 20 most frequent event types in the estimation dataset\n",
        "event_counts = key_data_estimation['eventtype'].value_counts().nlargest(20).reset_index()\n",
        "event_counts.columns = ['eventtype', 'count']\n",
        "\n",
        "# Plotting the distribution of the top 20 event types using Matplotlib\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='count', y='eventtype', data=event_counts, palette='viridis')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Top 20 Distribution of Event Types in Capital IQ Key Developments')\n",
        "plt.xlabel('Event Count')\n",
        "plt.ylabel('Event Type')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Js6S42Ywsq_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing In-Sample Data for Weekly Analysis\n",
        "\n",
        "In this section, we focus on filtering and preparing the weekly CRSP data for the in-sample period to facilitate further analysis.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Data Filtering**:\n",
        "   - Filtered out entries with null or negative prices and returns to ensure the quality and accuracy of the dataset.\n",
        "\n",
        "2. **Adjusted Price Calculation**:\n",
        "   - Calculated adjusted prices by dividing the raw price by the cumulative adjustment factor (`cfacpr`). This adjustment is crucial to account for stock splits, dividends, and other corporate actions.\n",
        "\n",
        "3. **Weekly Aggregation**:\n",
        "   - Grouped the data by `permco`, `ticker`, and weekly frequency (with the week ending on Friday). The last adjusted price of the week was selected, and weekly returns were calculated using the log-return formula, ensuring the data reflects weekly market movements accurately.\n",
        "\n",
        "4. **Data Structuring**:\n",
        "   - Renamed columns for better clarity and selected only the relevant columns (`permco`, `ticker`, `start_date`, `end_date`, `weekly_ret`, `adj_prc`) to focus on the necessary data for subsequent analysis."
      ],
      "metadata": {
        "id": "aB4s_tbYtlN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out null and negative prices and returns\n",
        "crsp_train_filtered = top_25_estimation_data[(top_25_estimation_data['prc'] > 0) & ~top_25_estimation_data['ret'].isna()]\n",
        "\n",
        "# Calculate adjusted price\n",
        "crsp_train_filtered['adj_prc'] = crsp_train_filtered['prc'] / crsp_train_filtered['cfacpr']\n",
        "\n",
        "# Set date column to datetime type\n",
        "crsp_train_filtered['date'] = pd.to_datetime(crsp_train_filtered['date'])\n",
        "\n",
        "# Group by permco, ticker, and weekly frequency ending on Friday\n",
        "weekly_data = crsp_train_filtered.groupby(['permco', 'ticker', pd.Grouper(key='date', freq='W-FRI')]).agg({\n",
        "    'adj_prc': lambda x: x.iloc[-1],  # Adjusted price at the end of the week\n",
        "    'ret': (lambda x: (np.exp(np.sum(np.log(1 + x))) - 1) * 100)  # Weekly return calculation\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "weekly_data.rename(columns={'date': 'end_date', 'ret': 'weekly_ret'}, inplace=True)\n",
        "\n",
        "# Add start_date column (beginning of the week)\n",
        "weekly_data['start_date'] = weekly_data['end_date'] - pd.DateOffset(days=4)\n",
        "\n",
        "# Select relevant columns\n",
        "weekly_data_crsp_insample = weekly_data[['permco', 'ticker', 'start_date', 'end_date', 'weekly_ret', 'adj_prc']]\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "weekly_data_crsp_insample.head()"
      ],
      "metadata": {
        "id": "p3DHmGUzsqmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Price Direction for In-Sample Data\n",
        "\n",
        "This section focuses on determining the weekly price movement direction based on adjusted prices within the in-sample dataset.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Direction Calculation**:\n",
        "   - Calculated the price direction by computing the difference in adjusted prices (`adj_prc`) for each `permco` and `ticker`. If the difference was negative, the direction was marked as 'Down', otherwise 'Up'. This classification helps in understanding the weekly price trends.\n",
        "\n",
        "2. **DataFrame Structuring**:\n",
        "   - The final DataFrame, `result_df`, includes the relevant columns: `permco`, `ticker`, `start_date`, `end_date`, `weekly_ret`, `adj_prc`, and the calculated `direction`, which will be used for further analysis."
      ],
      "metadata": {
        "id": "ziq2Msb5tyXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate direction based on adjusted prices\n",
        "weekly_data_crsp_insample['direction'] = weekly_data_crsp_insample.groupby(['permco', 'ticker'])['adj_prc'].diff().apply(lambda x: 'Down' if x < 0 else 'Up')\n",
        "\n",
        "# Display the resulting DataFrame with direction\n",
        "result_df = weekly_data_crsp_insample[['permco', 'ticker', 'start_date', 'end_date', 'weekly_ret', 'adj_prc', 'direction']]\n",
        "result_df.head()"
      ],
      "metadata": {
        "id": "qx0W3Lt9s0Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Out-Sample Data for Weekly Analysis\n",
        "\n",
        "This section outlines the steps taken to clean and prepare the out-sample data for analysis of weekly returns and adjusted prices.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Data Filtering**:\n",
        "   - Filtered out records with null or negative prices (`prc`) and returns (`ret`). This ensures that only valid and meaningful data is used for analysis.\n",
        "\n",
        "2. **Adjusted Price Calculation**:\n",
        "   - Calculated the adjusted price (`adj_prc`) by dividing the original price (`prc`) by the price adjustment factor (`cfacpr`). This adjustment accounts for events like stock splits and dividends, providing a more accurate measure of price.\n",
        "\n",
        "3. **Weekly Aggregation**:\n",
        "   - Grouped the data by `permco`, `ticker`, and weekly frequency ending on Friday. The last adjusted price of the week was selected, and the weekly return was calculated using a log-based approach to capture the compounded return over the week.\n",
        "\n",
        "4. **Data Structuring**:\n",
        "   - Renamed and added relevant columns, including `start_date` (beginning of the week) and `end_date` (end of the week). The final DataFrame, `weekly_data_crsp_outsample`, includes columns such as `permco`, `ticker`, `start_date`, `end_date`, `weekly_ret`, and `adj_prc`, setting the stage for further analysis."
      ],
      "metadata": {
        "id": "04DhyTX8uAmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out null and negative prices and returns\n",
        "crsp_train_filtered_out = top_25_out_sample_data[(top_25_out_sample_data['prc'] > 0) & ~top_25_out_sample_data['ret'].isna()]\n",
        "\n",
        "# Calculate adjusted price\n",
        "crsp_train_filtered_out['adj_prc'] = crsp_train_filtered_out['prc'] / crsp_train_filtered_out['cfacpr']\n",
        "\n",
        "# Set date column to datetime type\n",
        "crsp_train_filtered_out['date'] = pd.to_datetime(crsp_train_filtered_out['date'])\n",
        "\n",
        "# Group by permco, ticker, and weekly frequency ending on Friday\n",
        "weekly_data_out = crsp_train_filtered_out.groupby(['permco', 'ticker', pd.Grouper(key='date', freq='W-FRI')]).agg({\n",
        "    'adj_prc': lambda x: x.iloc[-1],  # Adjusted price at the end of the week\n",
        "    'ret': (lambda x: (np.exp(np.sum(np.log(1 + x))) - 1) * 100)  # Weekly return calculation\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "weekly_data_out.rename(columns={'date': 'end_date', 'ret': 'weekly_ret'}, inplace=True)\n",
        "\n",
        "# Add start_date column (beginning of the week)\n",
        "weekly_data_out['start_date'] = weekly_data_out['end_date'] - pd.DateOffset(days=4)\n",
        "\n",
        "# Select relevant columns\n",
        "weekly_data_crsp_outsample = weekly_data_out[['permco', 'ticker', 'start_date', 'end_date', 'weekly_ret', 'adj_prc']]\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "weekly_data_crsp_outsample.head()"
      ],
      "metadata": {
        "id": "6vr1MfkCt9Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Directional Analysis of Out-Sample Data\n",
        "\n",
        "This section describes the process used to calculate the direction of stock price movements based on adjusted prices and to prepare the final dataset for analysis.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Direction Calculation**:\n",
        "   - Calculated the direction of price movement by assessing the difference in adjusted prices (`adj_prc`) for each stock (`permco`, `ticker`). If the price decreased compared to the previous period, it was labeled as 'Down'; otherwise, it was labeled as 'Up'.\n",
        "\n",
        "2. **DataFrame Structuring**:\n",
        "   - Created a DataFrame, `result_outsample_df`, which includes the key variables such as `permco`, `ticker`, `start_date`, `end_date`, `weekly_ret`, `adj_prc`, and the calculated `direction`. This structured dataset provides a comprehensive view of the weekly returns and the direction of price movements, ready for further analysis."
      ],
      "metadata": {
        "id": "gBk_2Iaqun-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate direction based on adjusted prices\n",
        "weekly_data_crsp_outsample['direction'] = weekly_data_crsp_outsample.groupby(['permco', 'ticker'])['adj_prc'].diff().apply(lambda x: 'Down' if x < 0 else 'Up')\n",
        "\n",
        "# Display the resulting DataFrame with direction\n",
        "result_outsample_df = weekly_data_crsp_outsample[['permco', 'ticker', 'start_date', 'end_date', 'weekly_ret', 'adj_prc', 'direction']]\n",
        "result_outsample_df.head()"
      ],
      "metadata": {
        "id": "_VyGzmW9t88v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Weekly Returns by Ticker (In-Sample Data)\n",
        "\n",
        "This section illustrates the steps taken to visualize the weekly returns for each ticker in the in-sample data. The process involves grouping the data by ticker and creating subplots to display the weekly returns over time.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Grouping and Preparation**:\n",
        "   - Converted the `start_date` column to datetime format to ensure proper time series plotting.\n",
        "   - Grouped the data by `ticker` to prepare for individual plots for each company's stock.\n",
        "\n",
        "2. **Subplot Configuration**:\n",
        "   - Calculated the number of rows and columns needed for the subplots, depending on the number of tickers.\n",
        "   - Created a subplot grid, adjusting the layout to fit all tickers while ensuring clarity and readability.\n",
        "\n",
        "3. **Plotting**:\n",
        "   - Assigned unique colors to each ticker for visual distinction.\n",
        "   - Plotted the weekly returns (`weekly_ret`) against `start_date` for each ticker.\n",
        "   - Adjusted the subplot layout, hiding any unused plots, and added titles and labels to ensure the plot is informative.\n",
        "\n",
        "4. **Final Visualization**:\n",
        "   - The final plot, titled \"Weekly Returns by Ticker,\" offers a comprehensive view of the performance of each ticker over time within the in-sample period."
      ],
      "metadata": {
        "id": "IpkoQy3Bu9gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_data_crsp_insample['start_date'] = pd.to_datetime(weekly_data_crsp_insample['start_date'])\n",
        "\n",
        "# Group the data by ticker\n",
        "grouped_data = weekly_data_crsp_insample.groupby('ticker')\n",
        "\n",
        "# Calculate the number of rows and columns for subplots\n",
        "num_tickers = len(grouped_data)\n",
        "cols = 2\n",
        "rows = (num_tickers + cols - 1) // cols\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5), sharex=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Generate a list of unique colors\n",
        "colors = np.random.rand(num_tickers, 3)\n",
        "\n",
        "# Iterate over each ticker group and assign a unique color\n",
        "for idx, (ticker, data) in enumerate(grouped_data):\n",
        "    ax = axes[idx]\n",
        "    ax.plot(data['start_date'], data['weekly_ret'], color=colors[idx], label=ticker)\n",
        "    ax.set_title(ticker)\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Weekly Return')\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "    ax.legend()\n",
        "\n",
        "# Hide any unused subplots\n",
        "for i in range(num_tickers, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "# Adjust layout\n",
        "fig.tight_layout()\n",
        "fig.suptitle('Weekly Returns by Ticker', fontsize=16)\n",
        "fig.subplots_adjust(top=0.95)  # Adjust top to fit the title\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DzEPNpNTt8wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Weekly Returns by Ticker (Out-of-Sample Data)\n",
        "\n",
        "This section details the steps used to visualize the weekly returns for each ticker in the out-of-sample data. The process involves organizing the data by ticker and creating subplots for a comprehensive visualization of each stock's weekly performance.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - The `start_date` column was converted to datetime format to facilitate accurate time series plotting.\n",
        "   - The data was grouped by `ticker` to allow for individual plots for each company's stock returns.\n",
        "\n",
        "2. **Subplot Configuration**:\n",
        "   - Calculated the appropriate number of rows and columns required to accommodate the number of tickers in the dataset.\n",
        "   - A grid of subplots was created, with adjustments made to the layout to ensure clarity and effective use of space.\n",
        "\n",
        "3. **Plotting**:\n",
        "   - Each ticker was assigned a unique color to distinguish it visually in the plots.\n",
        "   - The weekly returns (`weekly_ret`) were plotted against `start_date` for each ticker, providing a detailed view of stock performance over time.\n",
        "   - The subplot layout was fine-tuned by hiding any unused plots and adding relevant titles and labels.\n",
        "\n",
        "4. **Final Visualization**:\n",
        "   - The final visualization, titled \"Weekly Returns by Ticker (Out-of-Sample),\" presents a clear and organized view of each ticker's performance throughout the out-of-sample period."
      ],
      "metadata": {
        "id": "GtDnzCYbvMhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_data_crsp_outsample['start_date'] = pd.to_datetime(weekly_data_crsp_outsample['start_date'])\n",
        "\n",
        "# Group the data by ticker\n",
        "grouped_data = weekly_data_crsp_outsample.groupby('ticker')\n",
        "\n",
        "# Calculate the number of rows and columns for subplots\n",
        "num_tickers = len(grouped_data)\n",
        "cols = 2\n",
        "rows = (num_tickers + cols - 1) // cols\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5), sharex=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Generate a list of unique colors\n",
        "colors = np.random.rand(num_tickers, 3)\n",
        "\n",
        "# Iterate over each ticker group and assign a unique color\n",
        "for idx, (ticker, data) in enumerate(grouped_data):\n",
        "    ax = axes[idx]\n",
        "    ax.plot(data['start_date'], data['weekly_ret'], color=colors[idx], label=ticker)\n",
        "    ax.set_title(ticker)\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Weekly Return')\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "    ax.legend()\n",
        "\n",
        "# Hide any unused subplots\n",
        "for i in range(num_tickers, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "# Adjust layout\n",
        "fig.tight_layout()\n",
        "fig.suptitle('Weekly Returns by Ticker (Out-of-Sample)', fontsize=16)\n",
        "fig.subplots_adjust(top=0.95)  # Adjust top to fit the title\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xKeyaIeQt8bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Key Development Data with CRSP Weekly Data (In-Sample)\n",
        "\n",
        "This section describes the process of preparing and merging key development data with CRSP weekly data for in-sample analysis. The goal is to align financial events with stock performance to enhance the predictive model's inputs.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Preparing the Key Development Data**:\n",
        "   - The `announcedate` column was converted to datetime format for accurate time-based operations.\n",
        "   - Each announcement date was mapped to the corresponding Friday of the week (`fri_of_week`).\n",
        "   - A new column, `next_fri_of_week`, was created to represent the Friday of the following week, providing a future reference point for event analysis.\n",
        "\n",
        "2. **Preparing the CRSP Weekly Data**:\n",
        "   - The `end_date` column in the CRSP dataset was also converted to datetime format.\n",
        "   - Each `end_date` was aligned with the corresponding Friday of the week (`fri_of_week`) to facilitate the merging process.\n",
        "\n",
        "3. **Merging the Datasets**:\n",
        "   - The Key Development Data was merged with the CRSP Weekly Data using `permco` and `fri_of_week` as common keys.\n",
        "   - The merge operation was performed as a left join, ensuring that all CRSP weekly records were retained even if no corresponding key development data was found.\n",
        "\n",
        "4. **Handling Missing Data**:\n",
        "   - Missing headlines in the merged dataset were filled with the placeholder 'No_Headlines' to maintain consistency.\n",
        "\n",
        "5. **Final Structuring**:\n",
        "   - The merged dataset was refined by selecting relevant columns, including `permco`, `ticker`, `start_date`, `end_date`, `weekly_ret`, `adj_prc`, `headline`, and `direction`.\n",
        "   - The resulting DataFrame, `in_sample_structured`, was then displayed for validation."
      ],
      "metadata": {
        "id": "4uSW1EY2vl0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare the Key Development Data\n",
        "key_data_estimation['announcedate'] = pd.to_datetime(key_data_estimation['announcedate'])\n",
        "key_data_estimation['fri_of_week'] = key_data_estimation['announcedate'].dt.to_period('W-FRI').apply(lambda r: r.end_time).dt.date\n",
        "key_data_estimation['next_fri_of_week'] = key_data_estimation['fri_of_week'] + timedelta(days=7)\n",
        "key_data_estimation['next_fri_of_week'] = key_data_estimation['next_fri_of_week'].astype(str)\n",
        "\n",
        "# Step 2: Prepare the CRSP Weekly Data\n",
        "weekly_data_crsp_insample['end_date'] = pd.to_datetime(weekly_data_crsp_insample['end_date'])\n",
        "weekly_data_crsp_insample['fri_of_week'] = weekly_data_crsp_insample['end_date'].dt.to_period('W-FRI').apply(lambda r: r.end_time).dt.date.astype(str)\n",
        "\n",
        "# Step 3: Merge the Key Development Data with the CRSP Weekly Data\n",
        "merged_data = pd.merge(weekly_data_crsp_insample, key_data_estimation,\n",
        "                       left_on=['permco', 'fri_of_week'],\n",
        "                       right_on=['permco', 'next_fri_of_week'],\n",
        "                       how='left')\n",
        "\n",
        "# Step 4: Fill Missing Headlines with 'No_Headlines'\n",
        "merged_data['headline'] = merged_data['headline'].fillna('No_Headlines')\n",
        "\n",
        "# Step 5: Select and reorder columns\n",
        "in_sample_structured = merged_data[['permco', 'ticker', 'start_date', 'end_date', 'weekly_ret', 'adj_prc', 'headline', 'direction']]\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "in_sample_structured.head()"
      ],
      "metadata": {
        "id": "fLssoVKLvPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Key Development Data with CRSP Weekly Data (Out-of-Sample)\n",
        "\n",
        "This section outlines the steps to merge key development data with CRSP weekly data for the out-of-sample period. This integration aims to link significant financial events to stock performance data for enhanced analysis and modeling.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Preparing the Key Development Data**:\n",
        "   - The `announcedate` column was converted to datetime format to ensure accurate temporal alignment.\n",
        "   - The corresponding Friday of each announcement date was identified and stored in the `fri_of_week` column.\n",
        "   - A new column, `next_fri_of_week`, was created to represent the following Friday, providing a forward-looking time reference for analyzing event impacts.\n",
        "\n",
        "2. **Preparing the CRSP Weekly Data**:\n",
        "   - The `end_date` in the CRSP dataset was also converted to datetime format for consistency.\n",
        "   - Each `end_date` was aligned with the appropriate Friday (`fri_of_week`) to facilitate the merge.\n",
        "\n",
        "3. **Merging the Datasets**:\n",
        "   - The Key Development Data was merged with the CRSP Weekly Data using `permco` and `fri_of_week` as join keys.\n",
        "   - A left join was employed to retain all CRSP weekly records, even if no corresponding key development data was available.\n",
        "\n",
        "4. **Handling Missing Data**:\n",
        "   - Missing headlines in the merged dataset were filled with 'No_Headlines' to ensure completeness and avoid gaps in the data.\n",
        "\n",
        "5. **Final Structuring**:\n",
        "   - The merged dataset was refined by selecting key columns, including `permco`, `ticker`, `start_date`, `end_date`, `weekly_ret`, `adj_prc`, `headline`, and `direction`.\n",
        "   - The resulting DataFrame, `out_sample_structured`, was displayed for review and further analysis."
      ],
      "metadata": {
        "id": "8-did-e6vxTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare the Key Development Data\n",
        "key_data_out_sample['announcedate'] = pd.to_datetime(key_data_out_sample['announcedate'])\n",
        "key_data_out_sample['fri_of_week'] = key_data_out_sample['announcedate'].dt.to_period('W-FRI').apply(lambda r: r.end_time).dt.date\n",
        "key_data_out_sample['next_fri_of_week'] = key_data_out_sample['fri_of_week'] + timedelta(days=7)\n",
        "key_data_out_sample['next_fri_of_week'] = key_data_out_sample['next_fri_of_week'].astype(str)\n",
        "\n",
        "# Step 2: Prepare the CRSP Weekly Data\n",
        "weekly_data_crsp_outsample['end_date'] = pd.to_datetime(weekly_data_crsp_outsample['end_date'])\n",
        "weekly_data_crsp_outsample['fri_of_week'] = weekly_data_crsp_outsample['end_date'].dt.to_period('W-FRI').apply(lambda r: r.end_time).dt.date.astype(str)\n",
        "\n",
        "# Step 3: Merge the Key Development Data with the CRSP Weekly Data\n",
        "merged_data = pd.merge(weekly_data_crsp_outsample, key_data_out_sample,\n",
        "                       left_on=['permco', 'fri_of_week'],\n",
        "                       right_on=['permco', 'next_fri_of_week'],\n",
        "                       how='left')\n",
        "\n",
        "# Step 4: Fill Missing Headlines with 'No_Headlines'\n",
        "merged_data['headline'] = merged_data['headline'].fillna('No_Headlines')\n",
        "\n",
        "# Step 5: Select and reorder columns\n",
        "out_sample_structured = merged_data[['permco', 'ticker', 'start_date', 'end_date', 'weekly_ret', 'adj_prc', 'headline', 'direction']]\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "out_sample_structured.head()"
      ],
      "metadata": {
        "id": "bVB3RIbavO-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization of Headlines for the Data\n",
        "\n",
        "This section outlines the steps involved in tokenizing and analyzing the textual data from stock-related headlines. By transforming the text into token counts, the analysis can identify the most common words across different stocks and in the dataset as a whole.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Exclusion of 'No_Headlines'**:\n",
        "   - The dataset was filtered to remove entries with 'No_Headlines', ensuring that only relevant text data is analyzed.\n",
        "\n",
        "2. **List of Stopwords**:\n",
        "   - A list of common English stopwords was defined using the NLTK library. These words (e.g., 'the', 'and', 'is') are excluded from the analysis to focus on more meaningful terms.\n",
        "\n",
        "3. **Tokenization Function**:\n",
        "   - A function `tokenize_text` was defined to convert the headlines into a format suitable for analysis. The function uses `CountVectorizer` from `sklearn` to transform the text into token counts, which are then aggregated to identify the most frequent words.\n",
        "\n",
        "4. **Tokenization by Ticker**:\n",
        "   - The headlines were tokenized separately for each stock (`ticker`). This allows for a detailed analysis of the most common terms associated with each specific stock.\n",
        "\n",
        "5. **Tokenization for All Stocks Together**:\n",
        "   - Additionally, all the headlines were tokenized collectively to provide an overview of the most frequent terms across the entire dataset.\n",
        "\n",
        "The resulting token counts offer valuable insights into the key themes and topics mentioned in the headlines, which can be further explored in subsequent analysis.\n"
      ],
      "metadata": {
        "id": "_Zu6BowuwGF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 'No_Headlines' from analysis\n",
        "in_sample_structured_NoHeadline = in_sample_structured[in_sample_structured['headline'] != 'No_Headlines']\n",
        "\n",
        "# List of stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def tokenize_text(headlines):\n",
        "    \"\"\"\n",
        "    Tokenizes the text in the headlines and returns the sum of token counts.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer(stop_words=stop_words)\n",
        "    X = vectorizer.fit_transform(headlines)\n",
        "    token_counts = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "    return token_counts.sum().sort_values(ascending=False)\n",
        "\n",
        "# Tokenize for each stock and all together\n",
        "token_counts_per_stock = {}\n",
        "for ticker in in_sample_structured_NoHeadline['ticker'].unique():\n",
        "    headlines = in_sample_structured_NoHeadline[in_sample_structured_NoHeadline['ticker'] == ticker]['headline']\n",
        "    token_counts_per_stock[ticker] = tokenize_text(headlines)\n",
        "\n",
        "# Tokenize for all stocks together\n",
        "all_headlines = in_sample_structured_NoHeadline['headline']\n",
        "total_token_counts = tokenize_text(all_headlines)"
      ],
      "metadata": {
        "id": "_9_vY50xwC8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Word Clouds for Stock Headlines\n",
        "\n",
        "This section demonstrates the process of generating word clouds, which visually represent the frequency of words in stock-related headlines. Word clouds help identify the most prominent terms associated with each stock and across the entire dataset.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Word Cloud Function**:\n",
        "   - The `plot_word_cloud` function was defined to create and display word clouds. It takes the text data and generates a word cloud, with more frequently occurring words displayed in larger fonts. The function also removes common stopwords to ensure the focus is on significant terms.\n",
        "\n",
        "2. **Subplot Layout Calculation**:\n",
        "   - The optimal layout for the subplots was calculated based on the number of word clouds to be generated. Each subplot corresponds to a specific stock or the entire dataset.\n",
        "\n",
        "3. **Figure Size Adjustment**:\n",
        "   - The figure's dimensions were adjusted to accommodate the number of subplots. The width and height were set to ensure that the word clouds are clearly visible and well-spaced.\n",
        "\n",
        "4. **Generating Word Clouds for Each Stock**:\n",
        "   - Word clouds were generated individually for each stock, allowing for a detailed visualization of the key terms mentioned in the headlines of that specific stock.\n",
        "\n",
        "5. **Generating a Word Cloud for All Headlines**:\n",
        "   - An additional word cloud was created using all the headlines in the dataset. This provides an overall view of the most frequently mentioned words across all stocks.\n",
        "\n",
        "6. **Hiding Unused Subplots**:\n",
        "   - Any extra subplot areas that were not used were hidden to maintain a clean and focused visual presentation.\n",
        "\n",
        "The resulting word clouds offer an insightful visual summary of the most discussed topics in the headlines, both for individual stocks and across the entire dataset."
      ],
      "metadata": {
        "id": "8WtI7jG5wWqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate and plot word cloud with specified size\n",
        "def plot_word_cloud(ax, text, title):\n",
        "    wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(text)\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(title)\n",
        "\n",
        "# Calculate optimal subplot layout\n",
        "num_plots = len(token_counts_per_stock) + 1  # Number of stocks + all headlines\n",
        "cols = 3  # Number of columns in the layout\n",
        "rows = (num_plots - 1) // cols + 1  # Calculate number of rows needed\n",
        "\n",
        "# Adjust figure size based on the number of subplots\n",
        "fig_width = 15  # Width of the figure\n",
        "fig_height = rows * 5  # Height of the figure, adjusted based on the number of rows\n",
        "\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(fig_width, fig_height))\n",
        "\n",
        "# Generate word clouds for each stock\n",
        "for i, ticker in enumerate(token_counts_per_stock.keys()):\n",
        "    row = i // cols\n",
        "    col = i % cols\n",
        "    text = ' '.join(in_sample_structured_NoHeadline[in_sample_structured_NoHeadline['ticker'] == ticker]['headline'])\n",
        "    plot_word_cloud(axs[row, col], text, f'Word Cloud for {ticker}')\n",
        "\n",
        "# Generate word cloud for all headlines together\n",
        "all_text = ' '.join(in_sample_structured_NoHeadline['headline'])\n",
        "plot_word_cloud(axs.flatten()[num_plots-1], all_text, 'Word Cloud for All Headlines')\n",
        "\n",
        "# Hide any extra subplots\n",
        "for i in range(num_plots, rows * cols):\n",
        "    axs.flatten()[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TNGkJK7CwCv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying Top Tokens in Stock Headlines\n",
        "\n",
        "This section outlines the steps taken to identify and display the most frequently occurring words (tokens) in the headlines associated with each stock and across the entire dataset.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Top Tokens Display Function**:\n",
        "   - The `display_top_tokens` function was defined to print out the most frequent tokens (words) in the headlines. This function takes in the token counts and a title for context, then outputs the top `n` tokens, where `n` is set to 10 by default.\n",
        "\n",
        "2. **Displaying Top Tokens for Each Stock**:\n",
        "   - The function was applied to the token counts of each stock. This allowed for the identification of the top 10 most frequent words in the headlines for each specific stock, providing insight into the key terms discussed for each company.\n",
        "\n",
        "3. **Displaying Top Tokens for All Headlines**:\n",
        "   - The function was also used to display the top tokens across all headlines in the dataset. This broader analysis highlights the most common themes or topics mentioned in the news across all companies.\n",
        "\n",
        "The identification and display of these top tokens provide a concise summary of the most significant words in the dataset, helping to understand the focus of news coverage for each stock and overall."
      ],
      "metadata": {
        "id": "EkVHEaK0wifE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display top tokens\n",
        "def display_top_tokens(token_counts, title, n=10):\n",
        "    print(f\"Top {n} tokens for {title}:\")\n",
        "    print(token_counts.head(n))\n",
        "    print()\n",
        "\n",
        "# Display top tokens for each stock\n",
        "for ticker, counts in token_counts_per_stock.items():\n",
        "    display_top_tokens(counts, ticker)\n",
        "\n",
        "# Display top tokens for all headlines together\n",
        "display_top_tokens(total_token_counts, 'All Headlines')"
      ],
      "metadata": {
        "id": "Zv0rM-cgwfvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Validation, Preprocessing, and Column Renaming\n",
        "\n",
        "This section details the steps involved in preparing and enhancing the in-sample and out-sample datasets for further analysis.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Loading the Datasets**:\n",
        "   - The `in_sample_structured` and `out_sample_structured` datasets were loaded into `in_sample_data` and `out_sample_data` respectively.\n",
        "\n",
        "2. **Renaming Columns**:\n",
        "   - A dictionary (`columns_rename_mapping`) was created to map the original column names to more descriptive names.\n",
        "   - The `rename` function was applied to both datasets to update the column names accordingly.\n",
        "\n",
        "3. **Ensuring Date Columns are in Datetime Format**:\n",
        "   - The `Start_Date` and `End_Date` columns in both datasets were converted to datetime format to ensure accurate date-based operations.\n",
        "\n",
        "4. **Checking for Missing Values and Duplicates**:\n",
        "   - Assertions were used to ensure that no missing values were present in either dataset.\n",
        "   - Duplicate rows, if any, were removed from the datasets.\n",
        "\n",
        "5. **Mapping the 'Direction' Column to Numerical Values**:\n",
        "   - The `Direction` column, originally containing values 'Up' or 'Down', was mapped to numerical values (1 for 'Up' and 0 for 'Down') for ease of analysis.\n",
        "\n",
        "6. **Generating Additional Features**:\n",
        "   - Additional features were generated as needed, such as the 5-period moving average of the adjusted price (`Adjusted_Price_MA5`).\n",
        "\n",
        "7. **Saving the Cleaned and Enhanced Datasets**:\n",
        "   - The cleaned and processed datasets were saved as CSV files for further use in analysis.\n",
        "\n",
        "These steps ensured that the datasets were well-prepared, with consistent formatting and no missing or duplicate values, ready for subsequent modeling and analysis."
      ],
      "metadata": {
        "id": "BlXzhkJwwwn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the in-sample and out-sample datasets\n",
        "in_sample_data = in_sample_structured\n",
        "out_sample_data = out_sample_structured\n",
        "\n",
        "# Rename columns\n",
        "columns_rename_mapping = {\n",
        "    'index': 'Index',\n",
        "    'permco': 'Permco',\n",
        "    'ticker': 'Ticker',\n",
        "    'start_date': 'Start_Date',\n",
        "    'end_date': 'End_Date',\n",
        "    'weekly_ret': 'Weekly_Return',\n",
        "    'adj_prc': 'Adjusted_Price',\n",
        "    'headline': 'Headline',\n",
        "    'direction': 'Direction'\n",
        "}\n",
        "\n",
        "in_sample_data.rename(columns=columns_rename_mapping, inplace=True)\n",
        "out_sample_data.rename(columns=columns_rename_mapping, inplace=True)\n",
        "\n",
        "# Ensure date columns are in datetime format\n",
        "in_sample_data['Start_Date'] = pd.to_datetime(in_sample_data['Start_Date'])\n",
        "in_sample_data['End_Date'] = pd.to_datetime(in_sample_data['End_Date'])\n",
        "out_sample_data['Start_Date'] = pd.to_datetime(out_sample_data['Start_Date'])\n",
        "out_sample_data['End_Date'] = pd.to_datetime(out_sample_data['End_Date'])\n",
        "\n",
        "# Check for missing values\n",
        "assert in_sample_data.isnull().sum().sum() == 0, \"In-sample data contains missing values\"\n",
        "assert out_sample_data.isnull().sum().sum() == 0, \"Out-sample data contains missing values\"\n",
        "\n",
        "# Check for duplicate rows and remove them\n",
        "in_sample_data = in_sample_data.drop_duplicates()\n",
        "out_sample_data = out_sample_data.drop_duplicates()\n",
        "\n",
        "# Convert 'Direction' to numerical values (e.g., 'Up' -> 1, 'Down' -> 0)\n",
        "direction_mapping = {'Up': 1, 'Down': 0}\n",
        "in_sample_data['Direction'] = in_sample_data['Direction'].map(direction_mapping)\n",
        "out_sample_data['Direction'] = out_sample_data['Direction'].map(direction_mapping)\n",
        "\n",
        "# Generate additional features if needed (e.g., moving averages, volatility)\n",
        "# Example: 5-period moving average of adjusted price\n",
        "in_sample_data['Adjusted_Price_MA5'] = in_sample_data['Adjusted_Price'].rolling(window=5).mean()\n",
        "out_sample_data['Adjusted_Price_MA5'] = out_sample_data['Adjusted_Price'].rolling(window=5).mean()\n",
        "\n",
        "# Save the cleaned and enhanced datasets\n",
        "in_sample_data.to_csv('/content/datasets/cleaned_in_sample_data.csv', index=False)\n",
        "out_sample_data.to_csv('/content/datasets/cleaned_out_sample_data.csv', index=False)\n",
        "\n",
        "print(\"Data validation, preprocessing, and column renaming complete.\")"
      ],
      "metadata": {
        "id": "iuGu_6rIwn8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Datasets and Preparing for Model Application\n",
        "\n",
        "This section outlines the initial steps for loading the in-sample and out-sample datasets, followed by the preparation required for applying various models.\n",
        "\n",
        "### Key Steps\n",
        "\n",
        "1. **Loading the Cleaned Datasets**:\n",
        "   - The cleaned datasets, `cleaned_in_sample_data.csv` and `cleaned_out_sample_data.csv`, were loaded into `in_sample_data` and `out_sample_data` respectively.\n",
        "\n",
        "2. **Model Name Mapping**:\n",
        "   - A dictionary (`model_name_mapping`) was created to map descriptive model names to their respective pretrained model identifiers.\n",
        "   - This mapping includes popular models such as BERT, RoBERTa, DistilBERT, DistilRoBERTa, and FinBERT.\n",
        "\n",
        "3. **Ensuring 'Start_Date' is in Datetime Format**:\n",
        "   - The `Start_Date` column in both datasets was converted to datetime format to ensure that the date-based operations, especially those crucial for time-series analysis and model training, are performed accurately."
      ],
      "metadata": {
        "id": "eBEMafoax0Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in-sample and out-sample datasets\n",
        "in_sample_data = pd.read_csv('/content/cleaned_in_sample_data.csv')\n",
        "out_sample_data = pd.read_csv('/content/cleaned_out_sample_data.csv')\n",
        "\n",
        "# Define model name mapping\n",
        "model_name_mapping = {\n",
        "    'bert': 'bert-base-uncased',\n",
        "    'roberta': 'roberta-base',\n",
        "    'distilbert': 'distilbert-base-uncased',\n",
        "    'distilroberta': 'distilroberta-base',\n",
        "    'finbert': 'yiyanghkust/finbert-tone'\n",
        "}\n",
        "\n",
        "# Ensure 'Start_Date' is in datetime format\n",
        "in_sample_data['Start_Date'] = pd.to_datetime(in_sample_data['Start_Date'])\n",
        "out_sample_data['Start_Date'] = pd.to_datetime(out_sample_data['Start_Date'])"
      ],
      "metadata": {
        "id": "41NSG9FaxT-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Dataset Creation and Embedding Generation\n",
        "\n",
        "This section involves creating a custom dataset class and generating embeddings for text data using various pretrained models.\n",
        "\n",
        "### Custom Dataset Class\n",
        "\n",
        "- **CustomDataset Class**:\n",
        "  - A custom dataset class is defined to handle the encoded input data and corresponding labels.\n",
        "  - The class provides the necessary methods (`__getitem__` and `__len__`) to make it compatible with PyTorch's DataLoader, allowing for efficient batching and shuffling during training.\n",
        "\n",
        "### Data Preparation and Embedding Generation\n",
        "\n",
        "- **Data Preparation**:\n",
        "  - The `prepare_data` function tokenizes the text data (headlines) using a specified tokenizer, ensuring truncation and padding to a fixed maximum length.\n",
        "  - The labels are extracted from the DataFrame and prepared for model input.\n",
        "\n",
        "- **Embedding Generation**:\n",
        "  - The `generate_embeddings` function computes two types of embeddings for each text input:\n",
        "    - **CLS Embedding**: The embedding of the `[CLS]` token, which is commonly used for classification tasks.\n",
        "    - **Average Embedding**: The average of all token embeddings in the sequence, providing a more holistic representation of the input.\n",
        "  - The embeddings are generated in batches for efficiency and stored back into the DataFrame.\n",
        "\n",
        "### Implementation Steps\n",
        "\n",
        "1. **Device Setup**:\n",
        "   - The computation is configured to use a GPU if available; otherwise, it defaults to CPU.\n",
        "\n",
        "2. **Model Setup**:\n",
        "   - Pretrained models like BERT, RoBERTa, DistilBERT, DistilRoBERTa, and FinBERT are loaded, and their configurations are adjusted to output hidden states (required for extracting embeddings).\n",
        "\n",
        "3. **Embedding Computation**:\n",
        "   - For each model, the text data from both in-sample and out-sample datasets is processed to generate embeddings.\n",
        "   - These embeddings are stored within the respective DataFrames.\n",
        "\n",
        "4. **Saving the Embeddings**:\n",
        "   - The generated embeddings are saved as CSV files for each company identified by `Permco`, with separate files for training (in-sample) and testing (out-sample) data."
      ],
      "metadata": {
        "id": "8W4J5D8CyQhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def prepare_data(df, tokenizer, max_length=512):\n",
        "    encodings = tokenizer(list(df['Headline'].values), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
        "    labels = list(df['Direction'].values)\n",
        "    return encodings, labels\n",
        "\n",
        "def generate_embeddings(df, tokenizer, model, device, model_name, batch_size=16):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    embeddings = []\n",
        "    avg_embeddings = []\n",
        "\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        batch_texts = df['Headline'].values[i:i + batch_size]\n",
        "        encodings = tokenizer(list(batch_texts), truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "        encodings = {key: val.to(device) for key, val in encodings.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encodings)\n",
        "            hidden_states = outputs.hidden_states[-1]\n",
        "            cls_embeddings = hidden_states[:, 0, :].cpu().numpy()  # Use the CLS token embedding\n",
        "            avg_embeddings_batch = hidden_states.mean(dim=1).cpu().numpy()  # Average embeddings\n",
        "\n",
        "        embeddings.extend(cls_embeddings)\n",
        "        avg_embeddings.extend(avg_embeddings_batch)\n",
        "\n",
        "    df[f'{model_name}_embedding'] = [cls_emb.tolist() for cls_emb in embeddings]\n",
        "    df[f'{model_name}_avg_embedding'] = [avg_emb.tolist() for avg_emb in avg_embeddings]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Generate embeddings for in-sample and out-sample data\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for model_key, model_name in model_name_mapping.items():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
        "    model.config.output_hidden_states = True\n",
        "    model.to(device)\n",
        "\n",
        "    in_sample_data = generate_embeddings(in_sample_data, tokenizer, model, device, model_key)\n",
        "    out_sample_data = generate_embeddings(out_sample_data, tokenizer, model, device, model_key)\n",
        "\n",
        "# # Save the generated embeddings\n",
        "for permco in in_sample_data['Permco'].unique():\n",
        "    train_data = in_sample_data[in_sample_data['Permco'] == permco]\n",
        "    test_data = out_sample_data[out_sample_data['Permco'] == permco]\n",
        "    os.makedirs(f'/content/datasets/{permco}', exist_ok=True)\n",
        "    train_data.to_csv(f'/content/datasets/{permco}/train_embeddings.csv', index=False)\n",
        "    test_data.to_csv(f'/content/datasets/{permco}/test_embeddings.csv', index=False)"
      ],
      "metadata": {
        "id": "CuD37gY7xZmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Evaluation with Metrics Logging and Plotting\n",
        "\n",
        "This section covers the training and evaluation of sequence classification models using text embeddings. It includes custom callback functions for logging training metrics, as well as functions for preparing data, training models, and visualizing results.\n",
        "\n",
        "### Custom Callback for Metrics Logging\n",
        "\n",
        "- **MetricsCallback Class**:\n",
        "  - A custom callback class is defined to log training loss and evaluation metrics (e.g., accuracy) during the training process.\n",
        "  - The callback captures the loss at each logging step and the evaluation accuracy at each evaluation step.\n",
        "\n",
        "### Metric Computation and Evaluation\n",
        "\n",
        "- **`compute_metrics` Function**:\n",
        "  - This function computes accuracy, precision, recall, and F1-score based on the model's predictions.\n",
        "  - It uses `accuracy_score` and `precision_recall_fscore_support` from `sklearn` to calculate these metrics.\n",
        "\n",
        "- **`evaluate_and_get_logits` Function**:\n",
        "  - This function evaluates the model on a given dataset and retrieves the logits (model output before applying the softmax function) and the true labels.\n",
        "  - It uses a DataLoader to process the dataset in batches for efficiency.\n",
        "\n",
        "### Model Training and Fine-Tuning\n",
        "\n",
        "- **`train_and_evaluate_model` Function**:\n",
        "  - This function trains and evaluates a model using the provided training and evaluation datasets.\n",
        "  - It uses the `Trainer` class from the `transformers` library to handle the training loop, evaluation, and logging.\n",
        "  - The function also saves the trained model's predictions and confusion matrix to CSV files.\n",
        "\n",
        "- **Training Setup**:\n",
        "  - The training process is set up with specific arguments, including batch size, number of epochs, and evaluation strategy.\n",
        "  - Models are trained and evaluated for each company (`Permco`) in the dataset using different pretrained models (e.g., BERT, RoBERTa).\n",
        "\n",
        "### Plotting Training Metrics\n",
        "\n",
        "- **`plot_training_metrics` Function**:\n",
        "  - This function visualizes the training loss over time by plotting the loss at each training step.\n",
        "  - The plot helps in understanding the training process and detecting any potential issues such as overfitting.\n",
        "\n",
        "### Execution and Results Storage\n",
        "\n",
        "- **Training and Evaluation Loop**:\n",
        "  - The code iterates over all companies in the dataset, training a separate model for each one.\n",
        "  - For each company and model, the training and evaluation results are stored in a dictionary, which includes metrics like accuracy, precision, recall, and F1-score.\n",
        "  - The results are saved separately for each year of the evaluation data.\n",
        "\n",
        "- **Handling CUDA Memory Issues**:\n",
        "  - The code includes a mechanism to handle CUDA out-of-memory errors by skipping the problematic company and clearing the GPU memory cache."
      ],
      "metadata": {
        "id": "r6073u5-yvaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.train_logs = []\n",
        "        self.eval_metrics = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            self.train_logs.append({\n",
        "                'step': state.global_step,\n",
        "                'loss': logs.get('loss', None)\n",
        "            })\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics is not None:\n",
        "            self.eval_metrics.append({\n",
        "                'eval_step': state.global_step,\n",
        "                'eval_accuracy': metrics.get('eval_accuracy', None)\n",
        "            })\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "def evaluate_and_get_logits(model, dataset, device):\n",
        "    model.eval()\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=16)\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits.cpu().numpy()\n",
        "            all_logits.extend(logits)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_logits), np.array(all_labels)\n",
        "\n",
        "def train_and_evaluate_model(train_df, eval_df, model_name, tokenizer_name, permco, drive_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    train_encodings, train_labels = prepare_data(train_df, tokenizer)\n",
        "    eval_encodings, eval_labels = prepare_data(eval_df, tokenizer)\n",
        "\n",
        "    train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "    eval_dataset = CustomDataset(eval_encodings, eval_labels)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
        "    model.config.output_hidden_states = True\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    output_dir = f'{drive_path}/{permco}/{model_name}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    metrics_callback = MetricsCallback()\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        logging_steps=1,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='no',  # Disable saving checkpoints\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[metrics_callback]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "\n",
        "    eval_logits, eval_labels = evaluate_and_get_logits(model, eval_dataset, device)\n",
        "    confusion_mat = confusion_matrix(eval_labels, np.argmax(eval_logits, axis=-1))\n",
        "\n",
        "    eval_df['Predicted_Direction'] = np.argmax(eval_logits, axis=-1)\n",
        "    eval_df.to_csv(f'{output_dir}/fine_tuned_predictions.csv', index=False)\n",
        "\n",
        "    # tokenizer.save_pretrained(f'{output_dir}/tokenizer')\n",
        "    # model.save_pretrained(f'{output_dir}/model')\n",
        "\n",
        "    return eval_results, model, tokenizer, device, eval_logits, confusion_mat, metrics_callback\n",
        "\n",
        "def plot_training_metrics(metrics_callback):\n",
        "    train_steps = [log['step'] for log in metrics_callback.train_logs if 'step' in log]\n",
        "    train_loss = [log['loss'] for log in metrics_callback.train_logs if 'loss' in log]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    if train_steps and train_loss:\n",
        "        plt.plot(train_steps, train_loss, label='Training Loss', linestyle='-')\n",
        "        plt.xlabel('Training Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss Over Time')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "results_dict = {}\n",
        "drive_path = '/content/drive/MyDrive/models'\n",
        "\n",
        "permcos = in_sample_data['Permco'].unique()\n",
        "for permco in permcos:\n",
        "    print(f\"Training and saving model for {permco}...\")\n",
        "    company_train_df = in_sample_data[in_sample_data['Permco'] == permco]\n",
        "    company_test_df = out_sample_data[out_sample_data['Permco'] == permco]\n",
        "\n",
        "    if company_train_df.empty or company_test_df.empty:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        for model_key, model_name in model_name_mapping.items():\n",
        "            eval_results, model, tokenizer, device, eval_logits, confusion_mat, metrics_callback = train_and_evaluate_model(\n",
        "                company_train_df, company_test_df, model_name, model_name, permco, drive_path)\n",
        "\n",
        "            plot_training_metrics(metrics_callback)\n",
        "\n",
        "            if permco not in results_dict:\n",
        "                results_dict[permco] = {}\n",
        "\n",
        "            unique_years = sorted(company_test_df['Start_Date'].dt.year.unique())\n",
        "            for year in unique_years:\n",
        "                year_data = company_test_df[company_test_df['Start_Date'].dt.year == year]\n",
        "                if year not in results_dict[permco]:\n",
        "                    results_dict[permco][year] = {}\n",
        "\n",
        "                results_dict[permco][year][model_key] = {\n",
        "                    'accuracy': eval_results['eval_accuracy'],\n",
        "                    'precision': eval_results['eval_precision'],\n",
        "                    'recall': eval_results['eval_recall'],\n",
        "                    'f1': eval_results['eval_f1'],\n",
        "                    'permco': permco,\n",
        "                    'ticker': company_train_df['Ticker'].unique()[0],\n",
        "                    'confusion_matrix': confusion_mat\n",
        "                }\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if 'CUDA out' in str(e):\n",
        "            print(f\"Skipping {permco} due to CUDA out of memory error.\")\n",
        "            torch.cuda.empty_cache()\n",
        "            continue\n",
        "        else:\n",
        "            raise e"
      ],
      "metadata": {
        "id": "Ht1Nzx_hynCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Fine-Tuned Model Predictions\n",
        "\n",
        "This section describes the process of merging prediction results from fine-tuned models across different companies (`Permco`) and models. The merged dataset is then saved for further analysis.\n",
        "\n",
        "### Steps Involved\n",
        "\n",
        "1. **Directory Setup**:\n",
        "   - The base directory is defined, where the models are stored in a structured format under directories named after `Permco` values. Each `Permco` directory contains subdirectories for different models.\n",
        "\n",
        "2. **Data Aggregation**:\n",
        "   - The code iterates through each `Permco` directory and then through each model directory within it.\n",
        "   - For each model, it checks if the `fine_tuned_predictions.csv` file exists, which contains the prediction results.\n",
        "\n",
        "3. **Data Loading and Augmentation**:\n",
        "   - If the predictions file is found, it is loaded into a DataFrame.\n",
        "   - Additional columns are added to the DataFrame to indicate the corresponding `Permco` and model used for generating the predictions.\n",
        "\n",
        "4. **Data Concatenation**:\n",
        "   - All DataFrames from the different models and `Permco` directories are concatenated into a single DataFrame.\n",
        "\n",
        "5. **Saving the Merged Dataset**:\n",
        "   - The final merged DataFrame is saved as a CSV file for further analysis or reporting.\n"
      ],
      "metadata": {
        "id": "XSiz-mRazDla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/MyDrive/models'\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for permco_dir in os.listdir(base_dir):\n",
        "    permco_path = os.path.join(base_dir, permco_dir)\n",
        "    if os.path.isdir(permco_path):\n",
        "        for model_dir in os.listdir(permco_path):\n",
        "            model_path = os.path.join(permco_path, model_dir)\n",
        "            eval_df_path = os.path.join(model_path, 'fine_tuned_predictions.csv')\n",
        "            if os.path.exists(eval_df_path):\n",
        "                # Load the eval_df file and append it to the list\n",
        "                df = pd.read_csv(eval_df_path)\n",
        "                df['Permco'] = permco_dir  # Add a column to identify the permco\n",
        "                df['Model'] = model_dir  # Add a column to identify the model\n",
        "                dfs.append(df)\n",
        "\n",
        "merged_df = pd.concat(dfs, ignore_index=True)\n",
        "merged_df.to_csv('/content/merged_fine_tuned_results_df.csv', index=False)\n",
        "print(\"Merged merged_fine_tuned_results_df saved to /content/merged_fine_tuned_results_df.csv\")"
      ],
      "metadata": {
        "id": "A_v3b0xfy4Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## Analyzing Model Performance (Fine-Tuned): Average Accuracies, Yearwise Metrics, Visualisations and Best Model Identification\n",
        "\n",
        "This section outlines the procedures for evaluating and visualizing the performance of fine-tuned models across different companies and years. The focus is on analyzing metrics such as accuracy, precision, recall, and F1 score, along with generating confusion matrices.\n",
        "\n",
        "### Key Functions and Their Purposes\n",
        "\n",
        "1. **`plot_confusion_matrix`**:\n",
        "   - Plots a confusion matrix for a given `Permco`, model, and year.\n",
        "\n",
        "2. **`plot_confusion_matrices`**:\n",
        "   - Iterates through the results dictionary to generate and display confusion matrices for each model and year.\n",
        "\n",
        "3. **`plot_yearwise_metrics_tickers`**:\n",
        "   - Plots the year-wise performance metrics (accuracy, precision, recall, F1) for each model across different `Permcos`.\n",
        "\n",
        "4. **`plot_yearwise_metrics_avg`**:\n",
        "   - Averages the performance metrics across all `Permcos` and plots them year-wise for each model.\n",
        "\n",
        "5. **`find_best_model`**:\n",
        "   - Determines the best-performing model for each year by comparing accuracy metrics across all models.\n",
        "\n",
        "6. **`plot_results_table`**:\n",
        "   - Displays a summary table of the best models by year, highlighting the overall best model.\n",
        "\n",
        "7. **`plot_best_model_by_year`**:\n",
        "   - Visualizes the frequency of each model being selected as the best model by year.\n",
        "\n",
        "8. **`find_best_model_tickerwise`**:\n",
        "   - Identifies the best model for each `Permco` based on the average accuracy across all years.\n",
        "\n",
        "9. **`plot_best_model_by_ticker`**:\n",
        "   - Visualizes the distribution of the best models across all `Permcos` using a pie chart.\n",
        "\n",
        "### Summary of Plots and Analysis\n",
        "\n",
        "- **Year-wise Metrics by Ticker**: Shows the performance of each model across different years for each company.\n",
        "- **Year-wise Average Metrics**: Aggregates the metrics across companies to provide an overall performance trend.\n",
        "- **Best Model by Year**: Identifies which model performs the best for each year and presents a summary in a table format.\n",
        "- **Best Model by Ticker**: Identifies the best-performing model for each company and visualizes it in a pie chart.\n",
        "\n",
        "### Execution and Visualization\n",
        "\n",
        "- The above functions are executed to generate plots and tables that provide insights into the performance of various models. The results help in identifying trends, comparing models, and determining the most effective models for different scenarios.\n",
        "\n",
        "- **Confusion Matrices**: Detailed matrices are plotted to give a clear picture of the classification accuracy for each model.\n",
        "\n",
        "- **Performance Metrics**: The metrics are plotted both in aggregate and individually to showcase the strengths and weaknesses of each model."
      ],
      "metadata": {
        "id": "lu3pzLqc0LCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, model_name, permco, year):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for {permco} ({model_name}) - {year}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrices(results_dict):\n",
        "    for permco, years in results_dict.items():\n",
        "        for year, models in years.items():\n",
        "            for model_name, metrics in models.items():\n",
        "                if 'confusion_matrix' in metrics:\n",
        "                    plot_confusion_matrix(metrics['confusion_matrix'], model_name, permco, year)\n",
        "\n",
        "def plot_yearwise_metrics_tickers(results_dict):\n",
        "    model_metrics = {model_name: {} for model_name in model_name_mapping.keys()}\n",
        "\n",
        "    for permco, years in results_dict.items():\n",
        "        for year, models in years.items():\n",
        "            for model_name, metrics in models.items():\n",
        "                if year not in model_metrics[model_name]:\n",
        "                    model_metrics[model_name][year] = {metric: [] for metric in metrics.keys() if metric != 'confusion_matrix'}\n",
        "                for metric, value in metrics.items():\n",
        "                    if metric != 'confusion_matrix':\n",
        "                        try:\n",
        "                            model_metrics[model_name][year][metric].append(float(value))\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "    for permco in results_dict.keys():\n",
        "        for model_name in model_name_mapping.keys():\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "                years = sorted(model_metrics[model_name].keys())\n",
        "                values = [np.mean(model_metrics[model_name][year][metric]) if metric in model_metrics[model_name][year] and model_metrics[model_name][year][metric] else np.nan for year in years]\n",
        "                plt.plot(years, values, label=metric, marker='o')\n",
        "            plt.legend()\n",
        "            plt.title(f'Yearwise Performance Metrics for {model_name} ({permco})')\n",
        "            plt.xlabel('Year')\n",
        "            plt.ylabel('Score')\n",
        "            plt.xlim(min(years), max(years))\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "def plot_yearwise_metrics_avg(results_dict):\n",
        "    model_metrics = {model_name: {} for model_name in model_name_mapping.keys()}\n",
        "\n",
        "    for permco, years in results_dict.items():\n",
        "        for year, models in years.items():\n",
        "            for model_name, metrics in models.items():\n",
        "                if year not in model_metrics[model_name]:\n",
        "                    model_metrics[model_name][year] = {metric: [] for metric in metrics.keys() if metric != 'confusion_matrix'}\n",
        "                for metric, value in metrics.items():\n",
        "                    if metric != 'confusion_matrix':\n",
        "                        try:\n",
        "                            model_metrics[model_name][year][metric].append(float(value))\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "    for model_name, years in model_metrics.items():\n",
        "        for year, metrics in years.items():\n",
        "            for metric in metrics:\n",
        "                if metrics[metric]:\n",
        "                    model_metrics[model_name][year][metric] = np.mean(metrics[metric])\n",
        "                else:\n",
        "                    model_metrics[model_name][year][metric] = np.nan\n",
        "\n",
        "    for model_name, metrics_dict in model_metrics.items():\n",
        "        years = sorted(metrics_dict.keys())\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "            values = [metrics_dict[year][metric] for year in years if not np.isnan(metrics_dict[year][metric])]\n",
        "            valid_years = [year for year in years if not np.isnan(metrics_dict[year][metric])]\n",
        "            plt.plot(valid_years, values, label=metric, marker='o')\n",
        "        plt.legend()\n",
        "        plt.title(f'Yearwise Performance Metrics for {model_name}')\n",
        "        plt.xlabel('Year')\n",
        "        plt.ylabel('Score')\n",
        "        plt.xlim(min(valid_years), max(valid_years))\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "def find_best_model(results_dict):\n",
        "    all_years = sorted({year for permco in results_dict for year in results_dict[permco]})\n",
        "    all_models = sorted({model for permco in results_dict for year in results_dict[permco] for model in results_dict[permco][year]})\n",
        "\n",
        "    year_model_accuracy = {year: {model: [] for model in all_models} for year in all_years}\n",
        "\n",
        "    for permco in results_dict:\n",
        "        for year in results_dict[permco]:\n",
        "            for model in results_dict[permco][year]:\n",
        "                accuracy = results_dict[permco][year][model].get('accuracy', np.nan)\n",
        "                year_model_accuracy[year][model].append(accuracy)\n",
        "\n",
        "    year_model_avg_accuracy = {year: {model: np.nanmean(year_model_accuracy[year][model]) for model in all_models} for year in all_years}\n",
        "\n",
        "    year_df = pd.DataFrame(year_model_avg_accuracy).T\n",
        "    year_df['Best_Model'] = year_df.idxmax(axis=1)\n",
        "\n",
        "    overall_best_model_yearwise = year_df[all_models].mean().idxmax()\n",
        "\n",
        "    return year_df, overall_best_model_yearwise\n",
        "\n",
        "def plot_results_table(df, overall_best_model):\n",
        "    fig, ax = plt.subplots(figsize=(20, 12))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    table = pd.plotting.table(ax, df, loc='center', cellLoc='center', colWidths=[0.2] * len(df.columns))\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(14)\n",
        "    table.scale(1.5, 1.5)\n",
        "    plt.title(f'Best Models by Year\\nOverall Best Model: {overall_best_model}', fontsize=18)\n",
        "    plt.show()\n",
        "\n",
        "def plot_best_model_by_year(df):\n",
        "    best_model_counts = df['Best_Model'].value_counts()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    best_model_counts.plot(kind='bar', color=['blue', 'orange', 'green', 'red', 'yellow'])\n",
        "    plt.title('Best Model by Year (ignoring the company)', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Number of Times Selected as Best')\n",
        "    plt.show()\n",
        "\n",
        "def find_best_model_tickerwise(results_dict):\n",
        "    permcos = sorted(results_dict.keys())\n",
        "    all_models = sorted({model for permco in results_dict for year in results_dict[permco] for model in results_dict[permco][year]})\n",
        "\n",
        "    ticker_model_accuracy = {permco: {model: [] for model in all_models} for permco in permcos}\n",
        "\n",
        "    for permco in results_dict:\n",
        "        for year in results_dict[permco]:\n",
        "            for model in results_dict[permco][year]:\n",
        "                accuracy = results_dict[permco][year][model].get('accuracy', np.nan)\n",
        "                ticker_model_accuracy[permco][model].append(accuracy)\n",
        "\n",
        "    ticker_model_avg_accuracy = {permco: {model: np.nanmean(ticker_model_accuracy[permco][model]) for model in all_models} for permco in permcos}\n",
        "\n",
        "    ticker_df = pd.DataFrame(ticker_model_avg_accuracy).T\n",
        "    ticker_df['Best_Model'] = ticker_df.idxmax(axis=1)\n",
        "\n",
        "    overall_best_model_tickerwise = ticker_df[all_models].mean().idxmax()\n",
        "\n",
        "    return ticker_df, overall_best_model_tickerwise\n",
        "\n",
        "def plot_best_model_by_ticker(df):\n",
        "    best_model_counts = df['Best_Model'].value_counts()\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    best_model_counts.plot(kind='pie', autopct='%1.1f%%', colors=['blue', 'orange', 'green', 'red', 'yellow'])\n",
        "    plt.title('Best Model by Majority Company', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('')\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for fine-tuned model metrics\n",
        "plot_yearwise_metrics_tickers(results_dict)\n",
        "plot_yearwise_metrics_avg(results_dict)\n",
        "\n",
        "year_df, overall_best_model_yearwise = find_best_model(results_dict)\n",
        "plot_results_table(year_df, overall_best_model_yearwise)\n",
        "plot_best_model_by_year(year_df)\n",
        "\n",
        "ticker_df, overall_best_model_tickerwise = find_best_model_tickerwise(results_dict)\n",
        "plot_results_table(ticker_df, overall_best_model_tickerwise)\n",
        "plot_best_model_by_ticker(ticker_df)\n",
        "\n",
        "plot_confusion_matrices(results_dict)"
      ],
      "metadata": {
        "id": "yNvExapXyn9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rolling Window Analysis with Embeddings\n",
        "\n",
        "This section explains the implementation of a rolling window analysis using embeddings generated by various models. The objective is to evaluate the performance of these models in predicting market direction (up or down) based on historical data.\n",
        "\n",
        "### Key Functions and Workflow\n",
        "\n",
        "1. **`rolling_window_analysis_with_embeddings`**:\n",
        "   - This function performs logistic regression on the embedding data, evaluating the model's performance using metrics such as accuracy, precision, recall, and F1 score.\n",
        "   - It also generates a confusion matrix and stores the predicted directions.\n",
        "\n",
        "2. **Rolling Window Analysis Process**:\n",
        "   - The analysis is performed for each `Permco` (company identifier) by combining the in-sample and out-sample datasets.\n",
        "   - The data is split into a rolling window, where 10 years of data are used for training, and the next year is used for testing.\n",
        "   - The rolling window continues across all years available in the dataset, evaluating the model's performance in predicting the market direction for each year.\n",
        "\n",
        "3. **Iterating Over Models and Years**:\n",
        "   - The script iterates over different models (e.g., BERT, RoBERTa) and years, applying the rolling window methodology.\n",
        "   - For each iteration, the script checks for the existence of embeddings and skips the analysis if embeddings are missing.\n",
        "\n",
        "4. **Result Aggregation**:\n",
        "   - The results of the rolling window analysis are stored in a dictionary structure, organized by `Permco`, year, and model.\n",
        "   - Additionally, the predicted directions are compiled into a DataFrame for further analysis or visualization.\n",
        "\n",
        "### Execution Summary\n",
        "\n",
        "- **Data Preparation**:\n",
        "  - The combined dataset for each `Permco` is prepared by concatenating in-sample and out-sample data. The dataset is then sorted by date and split into training and testing periods.\n",
        "  \n",
        "- **Rolling Window Application**:\n",
        "  - For each `Permco` and model, the rolling window analysis is conducted across the available years.\n",
        "  - The function ensures that each training and testing set is of sufficient size, skipping any iteration where the data is insufficient.\n",
        "\n",
        "- **Result Storage**:\n",
        "  - The analysis results, including accuracy, precision, recall, F1 score, and confusion matrices, are stored in a dictionary.\n",
        "  - Predicted directions are saved in a CSV file for further examination.\n",
        "\n",
        "### Output\n",
        "\n",
        "- **`predicted_rolling_window_test_directions.csv`**:\n",
        "  - This file contains the predicted directions for each company and model, along with the true directions, allowing for a detailed comparison and analysis."
      ],
      "metadata": {
        "id": "gcr3W_QW0rRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_window_analysis_with_embeddings(train_data, test_data, model_key):\n",
        "    results = []\n",
        "\n",
        "    train_embeddings = np.array(train_data[f'{model_key}_avg_embedding'].tolist())\n",
        "    test_embeddings = np.array(test_data[f'{model_key}_avg_embedding'].tolist())\n",
        "\n",
        "    train_labels = train_data['Direction'].values\n",
        "    test_labels = test_data['Direction'].values\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(train_embeddings, train_labels)\n",
        "    predictions = model.predict(test_embeddings)\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predictions, average='binary')\n",
        "    cm = confusion_matrix(test_labels, predictions)\n",
        "\n",
        "    results.append({\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'predicted_direction': predictions.tolist()\n",
        "    })\n",
        "\n",
        "    return results\n",
        "\n",
        "rolling_window_results_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
        "predicted_directions_df_list = []\n",
        "\n",
        "permcos = in_sample_data['Permco'].unique()\n",
        "for permco in permcos:\n",
        "    print(f\"Performing rolling window analysis for {permco}...\")\n",
        "\n",
        "    try:\n",
        "        combined_data = pd.concat([in_sample_data[in_sample_data['Permco'] == permco],\n",
        "                                   out_sample_data[out_sample_data['Permco'] == permco]])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Skipping {permco} due to missing file.\")\n",
        "        continue\n",
        "\n",
        "    combined_data['Start_Date'] = pd.to_datetime(combined_data['Start_Date'])\n",
        "\n",
        "    for model_key in model_name_mapping.keys():\n",
        "        if f'{model_key}_avg_embedding' not in combined_data.columns:\n",
        "            print(f\"Skipping {permco} for model {model_key} due to missing embeddings.\")\n",
        "            continue\n",
        "\n",
        "        combined_data[f'{model_key}_avg_embedding'] = combined_data[f'{model_key}_avg_embedding'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "        unique_years = sorted(combined_data['Start_Date'].dt.year.unique())\n",
        "        print(f\"Unique years for {permco} and model {model_key}: {unique_years}\")\n",
        "\n",
        "        for i in range(10, len(unique_years) - 1):\n",
        "            train_years = unique_years[i-10:i]\n",
        "            test_year = unique_years[i]\n",
        "\n",
        "            print(f\"Training years for {permco}, model {model_key}, test year {test_year}: {train_years}\")\n",
        "\n",
        "            rolling_train_data = combined_data[combined_data['Start_Date'].dt.year.isin(train_years)]\n",
        "            rolling_test_data = combined_data[combined_data['Start_Date'].dt.year == test_year]\n",
        "\n",
        "            print(f\"Rolling train data length: {len(rolling_train_data)}\")\n",
        "            print(f\"Rolling test data length: {len(rolling_test_data)}\")\n",
        "\n",
        "            if rolling_train_data.empty or rolling_test_data.empty:\n",
        "                print(f\"Skipping year {test_year} for {permco} due to empty rolling train or test data.\")\n",
        "                continue\n",
        "\n",
        "            result = rolling_window_analysis_with_embeddings(rolling_train_data, rolling_test_data, model_key)[0]\n",
        "\n",
        "            rolling_window_results_dict[permco][test_year][model_key] = result\n",
        "\n",
        "            if len(result['predicted_direction']) != len(rolling_test_data):\n",
        "                print(f\"Length mismatch: {len(result['predicted_direction'])} vs {len(rolling_test_data)}\")\n",
        "                continue\n",
        "\n",
        "            predicted_directions_df = pd.DataFrame({\n",
        "                'Permco': [permco] * len(rolling_test_data),\n",
        "                'Date': rolling_test_data['Start_Date'].values,\n",
        "                'Ticker': rolling_test_data['Ticker'].values,\n",
        "                'True_Direction': rolling_test_data['Direction'].values,\n",
        "                'Predicted_Direction': result['predicted_direction'],\n",
        "                'Model': [model_key] * len(rolling_test_data)\n",
        "            })\n",
        "\n",
        "            predicted_directions_df_list.append(predicted_directions_df)\n",
        "\n",
        "if predicted_directions_df_list:\n",
        "    predicted_directions_df = pd.concat(predicted_directions_df_list, ignore_index=True)\n",
        "    predicted_directions_df.to_csv('predicted_rolling_window_test_directions.csv', index=False)\n",
        "else:\n",
        "    print(\"No predicted directions to concatenate.\")"
      ],
      "metadata": {
        "id": "HY6I--n60vOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flattening Nested Dictionaries and Converting Rolling Window Results to DataFrame\n",
        "\n",
        "This section details the process of flattening nested dictionaries and converting the rolling window analysis results into a structured DataFrame for easier analysis and export.\n",
        "\n",
        "### Key Functions and Workflow\n",
        "\n",
        "1. **`flatten_dict`**:\n",
        "   - This recursive function is designed to flatten a nested dictionary.\n",
        "   - It takes a dictionary as input and returns a new dictionary where nested keys are concatenated with a separator (e.g., an underscore) to create a flat structure.\n",
        "   - This is particularly useful for handling the complex, nested structures that arise from storing multiple metrics in a hierarchical format.\n",
        "\n",
        "2. **`rolling_results_to_df`**:\n",
        "   - This function processes the `rolling_window_results_dict`, which contains the results of the rolling window analysis.\n",
        "   - It iterates over the dictionary, flattening the metrics and combining them with the `permco`, `year`, and `model` identifiers.\n",
        "   - The results are then stored in a list of records, which is converted into a Pandas DataFrame.\n",
        "\n",
        "3. **Exporting Results**:\n",
        "   - The DataFrame containing the flattened rolling window results is saved to a CSV file for further analysis or reporting.\n",
        "\n",
        "### Execution Summary\n",
        "\n",
        "- **Flattening Nested Metrics**:\n",
        "  - The `flatten_dict` function is applied to each set of metrics within the `rolling_window_results_dict`.\n",
        "  - This ensures that all metrics, regardless of their original depth in the nested structure, are accessible as individual columns in the resulting DataFrame.\n",
        "\n",
        "- **Converting to DataFrame**:\n",
        "  - The `rolling_results_to_df` function systematically converts the entire results dictionary into a DataFrame, where each row corresponds to a unique combination of `permco`, `year`, and `model`.\n",
        "  - The DataFrame provides a structured and flat representation of the rolling window results, making it easy to analyze the data across different dimensions.\n",
        "\n",
        "- **Result Storage**:\n",
        "  - The final DataFrame is saved as a CSV file, `rolling_window_test_results.csv`, which can be used for further data exploration, visualization, or integration into reports.\n",
        "\n",
        "### Output\n",
        "\n",
        "- **`rolling_window_test_results.csv`**:\n",
        "  - This file contains the flattened and structured results of the rolling window analysis, with columns representing various metrics, `permco`, `year`, and `model`.\n",
        "  - The flat format makes it easier to perform cross-comparisons, aggregations, and other forms of analysis on the rolling window results."
      ],
      "metadata": {
        "id": "ZLYjhcvE0204"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_dict(d, parent_key='', sep='_'):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "def rolling_results_to_df(rolling_window_results_dict):\n",
        "    records = []\n",
        "    for permco, years in rolling_window_results_dict.items():\n",
        "        for year, models in years.items():\n",
        "            for model, metrics in models.items():\n",
        "                flattened_metrics = flatten_dict(metrics)\n",
        "                record = {'permco': permco, 'year': year, 'model': model}\n",
        "                record.update(flattened_metrics)\n",
        "                records.append(record)\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "rolling_results_df = rolling_results_to_df(rolling_window_results_dict)\n",
        "rolling_results_df.to_csv('/content/datasets/rolling_window_test_results.csv', index=False)"
      ],
      "metadata": {
        "id": "t-Ut5YrN1IQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Model Performance (Rolling Window on Base Models): Average Accuracies, Yearwise Metrics, Visualisations and Best Model Identification\n",
        "\n",
        "This section outlines the process of evaluating and visualizing the performance of various models used in rolling window analysis. The focus is on plotting average accuracies across years, comparing yearwise metrics for different models, and identifying the best-performing models both yearwise and tickerwise.\n",
        "\n",
        "### Key Functions and Workflow\n",
        "\n",
        "1. **Plotting Average Accuracies**:\n",
        "   - **`plot_average_accuracies(results_dict)`**:\n",
        "     - This function calculates and plots the average accuracies of different models across years.\n",
        "     - For each year, it aggregates the accuracies from all `permco`s and computes the mean accuracy for each model.\n",
        "     - The results are visualized using line plots, with each model represented by a distinct line, allowing for a clear comparison of model performance over time.\n",
        "\n",
        "2. **Yearwise Metrics for Each Model**:\n",
        "   - **`plot_all_metrics_each_model(results_dict)`**:\n",
        "     - This function focuses on plotting multiple performance metrics (accuracy, precision, recall, and F1-score) for each model, year by year.\n",
        "     - It provides a detailed view of how each model performs across different metrics, making it easier to identify trends and performance stability over time.\n",
        "\n",
        "   - **`plot_all_metrics_each_model_ticker(results_dict)`**:\n",
        "     - Similar to the previous function, but tailored to analyze the performance metrics for each `permco` separately.\n",
        "     - This function plots yearwise metrics for each model within a specific `permco`, offering a more granular view of the model's performance for individual companies.\n",
        "\n",
        "3. **Identifying the Best Model**:\n",
        "   - **`find_best_model(results_dict)`**:\n",
        "     - This function identifies the best-performing model for each year by comparing the average accuracy across all models.\n",
        "     - It generates a DataFrame that highlights which model performed best each year and also determines the overall best model across all years.\n",
        "\n",
        "   - **`plot_results_table(df, overall_best_model)`**:\n",
        "     - Once the best models are identified, this function visualizes the results in a tabular format, making it easy to understand which models excelled in which years.\n",
        "     - The table is displayed along with the overall best model, providing a clear summary of model performance.\n",
        "\n",
        "   - **`plot_best_model_by_year(df)`**:\n",
        "     - This function creates a bar plot showing the frequency of each model being selected as the best model across different years.\n",
        "     - It gives a quick visual summary of which models are consistently performing well over time.\n",
        "\n",
        "4. **Best Model by Ticker (Company)**:\n",
        "   - **`find_best_model_tickerwise(results_dict)`**:\n",
        "     - This function identifies the best model for each `permco` by analyzing model performance across different years.\n",
        "     - It produces a DataFrame that shows the best model for each company and also determines the overall best model tickerwise.\n",
        "\n",
        "   - **`plot_best_model_by_ticker(df)`**:\n",
        "     - This function visualizes the best model for each company using a pie chart.\n",
        "     - The pie chart provides an intuitive understanding of which models are the most effective across the majority of companies.\n",
        "\n",
        "### Execution Summary\n",
        "\n",
        "- **Average Accuracies**:\n",
        "  - The average accuracies of models are plotted across different years, highlighting the overall trends and stability of each model's performance over time.\n",
        "\n",
        "- **Yearwise Performance Metrics**:\n",
        "  - Detailed metrics (accuracy, precision, recall, F1-score) are plotted for each model, offering a comprehensive view of their strengths and weaknesses year by year.\n",
        "\n",
        "- **Best Model Identification**:\n",
        "  - The best models are identified both on a yearly basis and across companies, with results visualized in tables and plots for easy interpretation.\n",
        "\n",
        "- **Results Visualization**:\n",
        "  - The plots and tables provide clear and actionable insights into model performance, helping in the selection of the most robust models for different scenarios.\n"
      ],
      "metadata": {
        "id": "SQwlV89C1teU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_average_accuracies(results_dict):\n",
        "    years = sorted(set(year for permco in results_dict for year in results_dict[permco]))\n",
        "    avg_accuracies = {model: [] for model in model_name_mapping.keys()}\n",
        "\n",
        "    for year in years:\n",
        "        for model_name in model_name_mapping.keys():\n",
        "            accuracies = [results_dict[permco][year][model_name]['accuracy']\n",
        "                          for permco in results_dict\n",
        "                          if year in results_dict[permco] and model_name in results_dict[permco][year]]\n",
        "            avg_accuracies[model_name].append(np.mean(accuracies) if accuracies else np.nan)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for model_name, accuracies in avg_accuracies.items():\n",
        "        plt.plot(years, accuracies, label=model_name, marker='o')\n",
        "\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Average Accuracy')\n",
        "    plt.title('Average Accuracies of Models Across the Years')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_all_metrics_each_model(results_dict):\n",
        "    for model_name in model_name_mapping.keys():\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "        years = sorted(set(year for permco in results_dict for year in results_dict[permco]))\n",
        "        avg_metrics = {metric: [] for metric in metrics}\n",
        "\n",
        "        for year in years:\n",
        "            for metric in metrics:\n",
        "                values = [results_dict[permco][year][model_name][metric]\n",
        "                          for permco in results_dict\n",
        "                          if year in results_dict[permco] and model_name in results_dict[permco][year]]\n",
        "                avg_metrics[metric].append(np.mean(values) if values else np.nan)\n",
        "\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        for metric, values in avg_metrics.items():\n",
        "            plt.plot(years, values, label=metric, marker='o')\n",
        "\n",
        "        plt.xlabel('Year')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title(f'Yearwise Performance Metrics for {model_name.upper()}')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "def plot_all_metrics_each_model_ticker(results_dict):\n",
        "    for permco in results_dict:\n",
        "        for model_name in model_name_mapping.keys():\n",
        "            metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "            years = sorted(results_dict[permco].keys())\n",
        "            metric_values = {metric: [] for metric in metrics}\n",
        "\n",
        "            for year in years:\n",
        "                for metric in metrics:\n",
        "                    if model_name in results_dict[permco][year]:\n",
        "                        metric_values[metric].append(results_dict[permco][year][model_name][metric])\n",
        "                    else:\n",
        "                        metric_values[metric].append(np.nan)\n",
        "\n",
        "            plt.figure(figsize=(15, 8))\n",
        "            for metric, values in metric_values.items():\n",
        "                plt.plot(years, values, label=metric, marker='o')\n",
        "\n",
        "            plt.xlabel('Year')\n",
        "            plt.ylabel('Score')\n",
        "            plt.title(f'Yearwise Performance Metrics for {model_name.upper()} ({permco})')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "def find_best_model(results_dict):\n",
        "    all_years = sorted({year for permco in results_dict for year in results_dict[permco]})\n",
        "    all_models = sorted({model for permco in results_dict for year in results_dict[permco] for model in results_dict[permco][year]})\n",
        "\n",
        "    year_model_accuracy = {year: {model: [] for model in all_models} for year in all_years}\n",
        "\n",
        "    for permco in results_dict:\n",
        "        for year in results_dict[permco]:\n",
        "            for model in results_dict[permco][year]:\n",
        "                accuracy = results_dict[permco][year][model].get('accuracy', np.nan)\n",
        "                year_model_accuracy[year][model].append(accuracy)\n",
        "\n",
        "    year_model_avg_accuracy = {year: {model: np.nanmean(year_model_accuracy[year][model]) for model in all_models} for year in all_years}\n",
        "\n",
        "    year_df = pd.DataFrame(year_model_avg_accuracy).T\n",
        "    year_df['Best_Model'] = year_df.idxmax(axis=1)\n",
        "\n",
        "    overall_best_model_yearwise = year_df[all_models].mean().idxmax()\n",
        "\n",
        "    return year_df, overall_best_model_yearwise\n",
        "\n",
        "def plot_results_table(df, overall_best_model):\n",
        "    fig, ax = plt.subplots(figsize=(20, 12))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    table = pd.plotting.table(ax, df, loc='center', cellLoc='center', colWidths=[0.2] * len(df.columns))\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(14)\n",
        "    table.scale(1.5, 1.5)\n",
        "    plt.title(f'Best Models by Year\\nOverall Best Model: {overall_best_model}', fontsize=18)\n",
        "    plt.show()\n",
        "\n",
        "def plot_best_model_by_year(df):\n",
        "    best_model_counts = df['Best_Model'].value_counts()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    best_model_counts.plot(kind='bar', color=['blue', 'orange', 'green', 'red', 'yellow'])\n",
        "    plt.title('Best Model by Year (ignoring the company)', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Number of Times Selected as Best')\n",
        "    plt.show()\n",
        "\n",
        "def find_best_model_tickerwise(results_dict):\n",
        "    permcos = sorted(results_dict.keys())\n",
        "    all_models = sorted({model for permco in results_dict for year in results_dict[permco] for model in results_dict[permco][year]})\n",
        "\n",
        "    ticker_model_accuracy = {permco: {model: [] for model in all_models} for permco in permcos}\n",
        "\n",
        "    for permco in results_dict:\n",
        "        for year in results_dict[permco]:\n",
        "            for model in results_dict[permco][year]:\n",
        "                accuracy = results_dict[permco][year][model].get('accuracy', np.nan)\n",
        "                ticker_model_accuracy[permco][model].append(accuracy)\n",
        "\n",
        "    ticker_model_avg_accuracy = {permco: {model: np.nanmean(ticker_model_accuracy[permco][model]) for model in all_models} for permco in permcos}\n",
        "\n",
        "    ticker_df = pd.DataFrame(ticker_model_avg_accuracy).T\n",
        "    ticker_df['Best_Model'] = ticker_df.idxmax(axis=1)\n",
        "\n",
        "    overall_best_model_tickerwise = ticker_df[all_models].mean().idxmax()\n",
        "\n",
        "    return ticker_df, overall_best_model_tickerwise\n",
        "\n",
        "def plot_best_model_by_ticker(df):\n",
        "    best_model_counts = df['Best_Model'].value_counts()\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    best_model_counts.plot(kind='pie', autopct='%1.1f%%', colors=['blue', 'orange', 'green', 'red', 'yellow'])\n",
        "    plt.title('Best Model by Majority Company', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('')\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for rolling window analysis\n",
        "plot_average_accuracies(rolling_window_results_dict)\n",
        "plot_all_metrics_each_model(rolling_window_results_dict)\n",
        "plot_all_metrics_each_model_ticker(rolling_window_results_dict)\n",
        "\n",
        "# Find best models by year and ticker\n",
        "year_df, overall_best_model_yearwise = find_best_model(rolling_window_results_dict)\n",
        "plot_results_table(year_df, overall_best_model_yearwise)\n",
        "plot_best_model_by_year(year_df)\n",
        "\n",
        "ticker_df, overall_best_model_tickerwise = find_best_model_tickerwise(rolling_window_results_dict)\n",
        "plot_best_model_by_ticker(ticker_df)"
      ],
      "metadata": {
        "id": "74cCxFZv1IuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating the MSE and RMSE for Rolling Window (Embeddings + Logistic Regression) Predictions"
      ],
      "metadata": {
        "id": "eqkQtE1uNgTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/predicted_rolling_window_test_directions.csv')\n",
        "\n",
        "# Initialize dictionaries to store MAE and MSE\n",
        "rmse_results = {}\n",
        "mse_results = {}\n",
        "\n",
        "# Get unique models from the data\n",
        "models = data['Model'].unique()\n",
        "\n",
        "# Calculate MSE and RMSE for each model\n",
        "for model in models:\n",
        "    model_data = data[data['Model'] == model]\n",
        "    true_values = model_data['True_Direction']\n",
        "    predicted_values = model_data['Predicted_Direction']\n",
        "\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "\n",
        "    mse_results[model] = mse\n",
        "    rmse = np.sqrt(mse)\n",
        "    rmse_results[model] = rmse\n",
        "\n",
        "# Display the results\n",
        "print(\"\\nMSE for each model:\")\n",
        "for model, mse in mse_results.items():\n",
        "    print(f\"{model}: {mse}\")\n",
        "\n",
        "print(\"\\nRMSE for each model:\")\n",
        "for model, rmse in rmse_results.items():\n",
        "    print(f\"{model}: {rmse}\")"
      ],
      "metadata": {
        "id": "wR_GlYAI8vCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating the MSE and RMSE for Fine-Tuned Predictions"
      ],
      "metadata": {
        "id": "Wn2tt893NV3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/merged_fine_tuned_results_df.csv')\n",
        "\n",
        "# Initialize dictionaries to store MAE and MSE\n",
        "mae_results = {}\n",
        "mse_results = {}\n",
        "rmse_results = {}\n",
        "\n",
        "# Get unique models from the data\n",
        "models = data['Model'].unique()\n",
        "\n",
        "# Calculate MAE and MSE and RMSE for each model\n",
        "for model in models:\n",
        "    model_data = data[data['Model'] == model]\n",
        "    true_values = model_data['Direction']\n",
        "    predicted_values = model_data['Predicted_Direction']\n",
        "\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "\n",
        "    mae_results[model] = mae\n",
        "    mse_results[model] = mse\n",
        "    rmse_results[model] = np.sqrt(mse)\n",
        "\n",
        "print(\"\\nMSE for each model:\")\n",
        "for model, mse in mse_results.items():\n",
        "    print(f\"{model}: {mse}\")\n",
        "\n",
        "print(\"\\nRMSE for each model:\")\n",
        "for model, rmse in rmse_results.items():\n",
        "    print(f\"{model}: {rmse}\")"
      ],
      "metadata": {
        "id": "xFUqc-qg89F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Portfolio Construction and Analysis: Market Capitalization and Portfolio Returns\n",
        "\n",
        "This section outlines the process of constructing and analyzing a stock portfolio based on market capitalization. The workflow includes data preprocessing, market cap classification, and portfolio return calculations with and without transaction costs.\n",
        "\n",
        "### Key Steps in Portfolio Construction and Analysis\n",
        "\n",
        "1. **Loading and Preprocessing Market Capitalization Data**:\n",
        "   - **Data Loading**:\n",
        "     - The market capitalization data is loaded from a CSV file, with specific data types assigned to relevant columns to handle large datasets and potential errors.\n",
        "     - **`PRC`** and **`SHROUT`** columns are converted to numeric values, with errors forced to **NaN** to handle non-numeric data.\n",
        "\n",
        "   - **Data Cleaning**:\n",
        "     - Rows with **NaN** values in **`PRC`** or **`SHROUT`** columns are dropped.\n",
        "     - The data is filtered to include only NYSE stocks, identified by **`EXCHCD == 1`**.\n",
        "\n",
        "   - **Date Handling**:\n",
        "     - The **`date`** column is converted to datetime format, and timezone information is removed if necessary to ensure consistency.\n",
        "     - Rows where the date conversion failed are dropped.\n",
        "\n",
        "   - **Market Cap Calculation**:\n",
        "     - Market capitalization is calculated as **`PRC`** multiplied by **`SHROUT`**.\n",
        "\n",
        "   - **Stock Classification**:\n",
        "     - Stocks are classified as **Large** or **Small** based on whether their market capitalization is above or below the median for that date.\n",
        "     - The **`TICKER`** column is renamed to **`Ticker`** for consistency.\n",
        "\n",
        "2. **Loading and Preprocessing Portfolio Returns Data**:\n",
        "   - The returns data is loaded from a CSV file and preprocessed to calculate daily returns based on the adjusted price.\n",
        "   - The data is filtered to include only the relevant time period (2016-2024) and rows with valid daily returns.\n",
        "\n",
        "3. **Merging Market Cap Data with Returns Data**:\n",
        "   - The market capitalization data is merged with the returns data based on **`Start_Date`** and **`Ticker`**.\n",
        "   - Transaction costs are applied to the returns based on the stock classification (Large or Small).\n",
        "   - Adjusted returns are calculated by subtracting the transaction costs from the daily returns.\n",
        "\n",
        "4. **Aggregating Data by Week**:\n",
        "   - The daily returns are aggregated by week, and the returns are calculated for each week for both the original and adjusted returns.\n",
        "   - This weekly aggregation is crucial for analyzing portfolio performance over time.\n",
        "\n",
        "5. **Pivoting Data for Portfolio Analysis**:\n",
        "   - The data is pivoted to create a matrix where each row represents a week and each column represents a stock ticker. The values are the weekly returns.\n",
        "   - The same pivoting is done for both the original and adjusted returns.\n",
        "\n",
        "   - **Market Returns Calculation**:\n",
        "     - The market return is assumed to be the average of all returns for each period, providing a benchmark for portfolio performance.\n",
        "\n",
        "6. **Saving Results**:\n",
        "   - The weekly returns without transaction costs are saved to a CSV file.\n",
        "   - The weekly returns with transaction costs are also saved to a separate CSV file."
      ],
      "metadata": {
        "id": "0sDj2zAI2ixc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the data and specify dtype for relevant columns\n",
        "market_cap_data = pd.read_csv('/content/crsp_2016-2023.csv',\n",
        "                              dtype={'PRC': 'str', 'SHROUT': 'str'},\n",
        "                              on_bad_lines='skip')\n",
        "\n",
        "# Convert PRC and SHROUT to numeric, forcing errors to NaN\n",
        "market_cap_data['PRC'] = pd.to_numeric(market_cap_data['PRC'], errors='coerce')\n",
        "market_cap_data['SHROUT'] = pd.to_numeric(market_cap_data['SHROUT'], errors='coerce')\n",
        "\n",
        "# Drop rows where PRC or SHROUT couldn't be converted to numbers\n",
        "market_cap_data.dropna(subset=['PRC', 'SHROUT'], inplace=True)\n",
        "\n",
        "# Step 2: Filter for NYSE stocks (EXCHCD == 1)\n",
        "market_cap_data = market_cap_data[market_cap_data['EXCHCD'] == 1]\n",
        "\n",
        "# Step 3: Handle the date column and ensure consistency\n",
        "market_cap_data['date'] = pd.to_datetime(market_cap_data['date'], errors='coerce')\n",
        "\n",
        "# Remove timezone information to make all dates naive (if necessary)\n",
        "market_cap_data['date'] = market_cap_data['date'].dt.tz_localize(None)\n",
        "\n",
        "# Drop rows where date conversion failed\n",
        "market_cap_data.dropna(subset=['date'], inplace=True)\n",
        "\n",
        "# Step 4: Calculate Market Cap\n",
        "market_cap_data['Market_Cap'] = market_cap_data['PRC'] * market_cap_data['SHROUT']\n",
        "\n",
        "# Step 5: Classify Stocks Based on Market Cap\n",
        "market_cap_data['Classification'] = market_cap_data.groupby('date')['Market_Cap'].transform(\n",
        "    lambda x: np.where(x >= x.median(), 'Large', 'Small')\n",
        ")\n",
        "\n",
        "# Rename TICKER to Ticker\n",
        "market_cap_data.rename(columns={'TICKER': 'Ticker'}, inplace=True)\n",
        "\n",
        "# Step 6: Load Portfolio Returns Data\n",
        "returns_data = pd.read_csv('/content/cleaned_out_sample_data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "returns_data['Start_Date'] = pd.to_datetime(returns_data['Start_Date'])\n",
        "returns_data['Daily_Return'] = returns_data.groupby('Ticker')['Adjusted_Price'].pct_change()\n",
        "filtered_data = returns_data[(returns_data['Start_Date'] >= '2016-01-01') & (returns_data['Start_Date'] <= '2024-12-31')]\n",
        "filtered_data = filtered_data.dropna(subset=['Daily_Return'])\n",
        "\n",
        "# Step 7: Merge Market Cap Data with Returns Data\n",
        "merged_data = filtered_data.merge(market_cap_data[['date', 'Ticker', 'Classification']],\n",
        "                                  left_on=['Start_Date', 'Ticker'],\n",
        "                                  right_on=['date', 'Ticker'],\n",
        "                                  how='left')\n",
        "\n",
        "# Step 8: Apply Transaction Costs\n",
        "merged_data['Transaction_Cost'] = np.where(\n",
        "    merged_data['Classification'] == 'Large', 11.21 / 10000, 21.27 / 10000\n",
        ")\n",
        "merged_data['Adjusted_Return'] = merged_data['Daily_Return'] - merged_data['Transaction_Cost']\n",
        "\n",
        "# Step 9: Aggregate Data by Week\n",
        "merged_data['Week'] = merged_data['Start_Date'].dt.to_period('W')\n",
        "aggregated_data = merged_data.groupby(['Week', 'Ticker']).agg({'Daily_Return': lambda x: (1 + x).prod() - 1}).reset_index()\n",
        "adjusted_aggregated_data = merged_data.groupby(['Week', 'Ticker']).agg({'Adjusted_Return': lambda x: (1 + x).prod() - 1}).reset_index()\n",
        "\n",
        "# Step 10: Pivot Data for Portfolio Analysis\n",
        "returns_data_pivot = aggregated_data.pivot(index='Week', columns='Ticker', values='Daily_Return')\n",
        "adjusted_returns_data_pivot = adjusted_aggregated_data.pivot(index='Week', columns='Ticker', values='Adjusted_Return')\n",
        "\n",
        "returns_data_pivot.index = returns_data_pivot.index.to_timestamp()\n",
        "adjusted_returns_data_pivot.index = adjusted_returns_data_pivot.index.to_timestamp()\n",
        "\n",
        "# Assuming the market return is the average of all returns for each period\n",
        "market_returns = returns_data_pivot.mean(axis=1)\n",
        "adjusted_market_returns = adjusted_returns_data_pivot.mean(axis=1)\n",
        "\n",
        "# Save the returns without transaction costs to a CSV file\n",
        "returns_data_pivot.to_csv('/content/returns_without_transaction_costs.csv')\n",
        "\n",
        "# Save the returns with transaction costs to a CSV file\n",
        "adjusted_returns_data_pivot.to_csv('/content/returns_with_transaction_costs.csv')"
      ],
      "metadata": {
        "id": "EULgDQoD1J4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Functions for Portfolio Construction and Analysis\n",
        "\n",
        "This section details the common functions used for portfolio construction and analysis in both rolling-window base models and fine-tuned models. These functions are essential for calculating portfolio returns, assessing performance, and evaluating the effectiveness of different investment strategies.\n",
        "\n",
        "### Key Functions for Portfolio Analysis\n",
        "\n",
        "1. **Calculate Returns for a Portfolio**:\n",
        "    - **`calculate_returns(data, weights)`**:\n",
        "        - This function calculates the weighted returns of a portfolio based on the given data and weights.\n",
        "        - **`data`**: A DataFrame where each column represents a stock's returns.\n",
        "        - **`weights`**: An array of weights assigned to each stock.\n",
        "        - The function multiplies the stock returns by their respective weights and sums them to get the overall portfolio returns.\n",
        "\n",
        "2. **Portfolio Returns Calculation Based on Predictions**:\n",
        "    - **`calculate_portfolio_returns(data, predictions, N=10)`**:\n",
        "        - This function calculates the returns for portfolios constructed based on predictions.\n",
        "        - **`N`**: The number of top and bottom stocks to include in the long (L) and short (S) portfolios.\n",
        "        - **Equally Weighted (EW) Portfolios**:\n",
        "            - **`ew_l`**: Returns for the top **`N`** stocks.\n",
        "            - **`ew_s`**: Returns for the bottom **`N`** stocks.\n",
        "            - **`ew_ls`**: Long-short portfolio returns (top **`N`** minus bottom **`N`**).\n",
        "        - **Value Weighted (VW) Portfolios**:\n",
        "            - **`vw_l`**: Returns for the top **`N`** stocks, weighted by their prediction scores.\n",
        "            - **`vw_s`**: Returns for the bottom **`N`** stocks, weighted by their prediction scores.\n",
        "            - **`vw_ls`**: Long-short portfolio returns (top **`N`** minus bottom **`N`**).\n",
        "\n",
        "3. **Sharpe Ratio Calculation**:\n",
        "    - **`calculate_sharpe_ratio(returns, risk_free_rate=0)`**:\n",
        "        - This function calculates the Sharpe ratio, which is a measure of risk-adjusted return.\n",
        "        - **`returns`**: The portfolio returns.\n",
        "        - **`risk_free_rate`**: The risk-free rate, defaulting to 0.\n",
        "        - The function calculates the excess returns (returns minus risk-free rate) and divides the mean excess return by its standard deviation to obtain the Sharpe ratio.\n",
        "\n",
        "4. **Process Model for Portfolio Construction**:\n",
        "    - **`process_model(model, predicted_directions, returns_data, market_returns)`**:\n",
        "        - This function processes a specific model to calculate the portfolio returns based on the model's predictions.\n",
        "        - **`model`**: The model being evaluated.\n",
        "        - **`predicted_directions`**: DataFrame containing the predicted directions for each stock.\n",
        "        - **`returns_data`**: DataFrame containing the actual returns data for the stocks.\n",
        "        - **`market_returns`**: Benchmark market returns for comparison.\n",
        "        - The function filters the relevant data for the given model, calculates predicted returns, and constructs portfolios (both equally weighted and value weighted).\n",
        "        - It returns the portfolio returns for both equally weighted and value weighted strategies, along with the overall portfolio returns and market returns."
      ],
      "metadata": {
        "id": "3tydFD2t28QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Perform Portfolio Analysis\n",
        "def calculate_returns(data, weights):\n",
        "    weighted_returns = data.mul(weights, axis=1)\n",
        "    portfolio_returns = weighted_returns.sum(axis=1)\n",
        "    return portfolio_returns\n",
        "\n",
        "def calculate_portfolio_returns(data, predictions, N=10):\n",
        "    top_n_indices = predictions.argsort()[-N:][::-1]\n",
        "    bottom_n_indices = predictions.argsort()[:N]\n",
        "\n",
        "    ew_weights = np.array([1/N] * N)\n",
        "    vw_weights_top = predictions[top_n_indices] / predictions[top_n_indices].sum()\n",
        "    vw_weights_bottom = predictions[bottom_n_indices] / predictions[bottom_n_indices].sum()\n",
        "\n",
        "    ew_l = calculate_returns(data.iloc[:, top_n_indices], ew_weights)\n",
        "    ew_s = calculate_returns(data.iloc[:, bottom_n_indices], ew_weights)\n",
        "    ew_ls = ew_l - ew_s\n",
        "\n",
        "    vw_l = calculate_returns(data.iloc[:, top_n_indices], vw_weights_top)\n",
        "    vw_s = calculate_returns(data.iloc[:, bottom_n_indices], vw_weights_bottom)\n",
        "    vw_ls = vw_l - vw_s\n",
        "\n",
        "    returns = {\n",
        "        'EW L': ew_l,\n",
        "        'EW S': ew_s,\n",
        "        'EW LS': ew_ls,\n",
        "        'VW L': vw_l,\n",
        "        'VW S': vw_s,\n",
        "        'VW LS': vw_ls\n",
        "    }\n",
        "    return returns\n",
        "\n",
        "def calculate_sharpe_ratio(returns, risk_free_rate=0):\n",
        "    excess_returns = returns - risk_free_rate\n",
        "    return excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "def process_model(model, predicted_directions, returns_data, market_returns):\n",
        "    model_data = predicted_directions[predicted_directions['Model'] == model]\n",
        "    model_tickers = model_data['Ticker'].unique()\n",
        "    model_returns_data = returns_data.loc[:, returns_data.columns.isin(model_tickers)]\n",
        "\n",
        "    if model_returns_data.empty:\n",
        "        print(f\"No data available for model: {model}\")\n",
        "        return None, None, None\n",
        "\n",
        "    predicted_returns = model_data.groupby('Ticker')['Predicted_Direction'].mean()\n",
        "    portfolio_returns = calculate_portfolio_returns(model_returns_data, predicted_returns)\n",
        "\n",
        "    ew_l_returns = portfolio_returns['EW L']\n",
        "    ew_s_returns = portfolio_returns['EW S']\n",
        "    ew_ls_returns = portfolio_returns['EW LS']\n",
        "\n",
        "    vw_l_returns = portfolio_returns['VW L']\n",
        "    vw_s_returns = portfolio_returns['VW S']\n",
        "    vw_ls_returns = vw_l_returns - vw_s_returns\n",
        "\n",
        "    return ew_l_returns, ew_s_returns, ew_ls_returns, vw_l_returns, vw_s_returns, vw_ls_returns, portfolio_returns, market_returns"
      ],
      "metadata": {
        "id": "tmigA2sn2npd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Un-Tuned Model Predictions for Portfolio Analysis\n",
        "\n",
        "In this section, we detail the steps for processing and analyzing the un-tuned model predictions using portfolio construction techniques. This involves calculating portfolio returns, Sharpe ratios, and visualizing cumulative returns over time.\n",
        "\n",
        "### Key Steps in the Process\n",
        "\n",
        "1. **Loading and Iterating Over Un-Tuned Model Predictions**:\n",
        "    - **`un_tuned_predicted_directions`**: The DataFrame containing the predicted directions for each stock based on un-tuned models is loaded from a CSV file.\n",
        "    - **`un_tuned_models`**: Unique model identifiers are extracted to iterate over each model’s predictions.\n",
        "    - For each model, the following steps are performed to calculate portfolio returns and metrics.\n",
        "\n",
        "2. **Processing Each Model**:\n",
        "    - **`process_model()`**: This function is used to calculate portfolio returns for both equally weighted (EW) and value weighted (VW) portfolios:\n",
        "        - **EW and VW Long**: Returns for the top N stocks.\n",
        "        - **EW and VW Short**: Returns for the bottom N stocks.\n",
        "        - **EW and VW Long-Short (LS)**: Returns for a portfolio that goes long on the top N stocks and short on the bottom N stocks.\n",
        "    - **Transaction Costs**: Portfolio returns with and without transaction costs are considered, though in this case, the adjusted returns are commented out.\n",
        "\n",
        "3. **Calculating and Storing Sharpe Ratios**:\n",
        "    - **`calculate_sharpe_ratio()`**: Sharpe ratios are calculated for each portfolio configuration (EW L, EW S, EW LS, VW L, VW S, VW LS) as well as for the market benchmark.\n",
        "    - The results are stored in a list of dictionaries, where each dictionary contains the Sharpe ratios for a specific model.\n",
        "\n",
        "4. **Visualizing Cumulative Returns**:\n",
        "    - **Cumulative Returns Plot**: For each model, cumulative returns are plotted over time to visualize the performance of the portfolios.\n",
        "        - The cumulative log returns for each portfolio and the market benchmark are plotted, with different line styles distinguishing between unadjusted and adjusted returns.\n",
        "\n",
        "5. **Saving the Sharpe Ratios**:\n",
        "    - **Conversion to DataFrame**: The list of Sharpe ratios is converted into a DataFrame for easier analysis and saving.\n",
        "    - **Saving to CSV**: The resulting DataFrame containing Sharpe ratios for all models is saved to a CSV file for further analysis."
      ],
      "metadata": {
        "id": "8_djFUCK3PYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Process each model's un-tuned predictions\n",
        "un_tuned_predicted_directions = pd.read_csv('/content/predicted_rolling_window_test_directions.csv')\n",
        "un_tuned_models = un_tuned_predicted_directions['Model'].unique()\n",
        "un_tuned_all_sharpe_ratios = []\n",
        "\n",
        "for model in un_tuned_models:\n",
        "    ew_l, ew_s, ew_ls, vw_l, vw_s, vw_ls, portfolio_returns, market_returns = process_model(\n",
        "        model, un_tuned_predicted_directions, returns_data_pivot, market_returns)\n",
        "    adj_ew_l, adj_ew_s, adj_ew_ls, adj_vw_l, adj_vw_s, adj_vw_ls, adj_portfolio_returns, adj_market_returns = process_model(\n",
        "        model, un_tuned_predicted_directions, adjusted_returns_data_pivot, adjusted_market_returns)\n",
        "\n",
        "    if ew_l is None or vw_l is None:\n",
        "        continue\n",
        "\n",
        "    model_sharpe_ratios = {\n",
        "        'Model': model,\n",
        "        'EW L': calculate_sharpe_ratio(ew_l),\n",
        "        'EW S': calculate_sharpe_ratio(ew_s),\n",
        "        'EW LS': calculate_sharpe_ratio(ew_ls),\n",
        "        'VW L': calculate_sharpe_ratio(vw_l),\n",
        "        'VW S': calculate_sharpe_ratio(vw_s),\n",
        "        'VW LS': calculate_sharpe_ratio(vw_ls),\n",
        "        'Market': calculate_sharpe_ratio(market_returns),\n",
        "        # 'Adj EW L': calculate_sharpe_ratio(adj_ew_l),\n",
        "        # 'Adj EW S': calculate_sharpe_ratio(adj_ew_s),\n",
        "        # 'Adj EW LS': calculate_sharpe_ratio(adj_ew_ls),\n",
        "        # 'Adj VW L': calculate_sharpe_ratio(adj_vw_l),\n",
        "        # 'Adj VW S': calculate_sharpe_ratio(adj_vw_s),\n",
        "        # 'Adj VW LS': calculate_sharpe_ratio(adj_vw_ls),\n",
        "        # 'Adj Market': calculate_sharpe_ratio(adj_market_returns)\n",
        "    }\n",
        "\n",
        "    un_tuned_all_sharpe_ratios.append(model_sharpe_ratios)\n",
        "\n",
        "    # Plotting Cumulative Returns\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for label, returns in portfolio_returns.items():\n",
        "        plt.plot((1 + returns).cumprod() - 1, label=label)\n",
        "    # for label, returns in adj_portfolio_returns.items():\n",
        "    #     plt.plot((1 + returns).cumprod() - 1, label=label, linestyle='--')\n",
        "    plt.plot((1 + market_returns).cumprod() - 1, label='Market', linestyle=':')\n",
        "    # plt.plot((1 + adj_market_returns).cumprod() - 1, label='Adj Market', linestyle='-.')\n",
        "    plt.title(f'Cumulative Weekly Portfolio Returns Over Time for Un-Tuned {model}')\n",
        "    plt.xlabel('Date', fontsize=14)\n",
        "    plt.ylabel('Cumulative Log Returns', fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Step 13: Convert Sharpe ratios to DataFrame and save\n",
        "un_tuned_sharpe_ratios_df = pd.DataFrame(un_tuned_all_sharpe_ratios)\n",
        "print(un_tuned_sharpe_ratios_df)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "un_tuned_sharpe_ratios_df.to_csv('/content/un_tuned_sharpe_ratios.csv', index=False)"
      ],
      "metadata": {
        "id": "KXBgTDvf1JUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Fine-Tuned Model Predictions for Portfolio Analysis\n",
        "\n",
        "This section outlines the process of analyzing fine-tuned model predictions by constructing portfolios, calculating Sharpe ratios, and visualizing cumulative returns over time.\n",
        "\n",
        "### Key Steps in the Process\n",
        "\n",
        "1. **Loading and Iterating Over Fine-Tuned Model Predictions**:\n",
        "    - **`fine_tuned_predicted_directions`**: The DataFrame containing the predicted directions for each stock based on fine-tuned models is loaded from a CSV file.\n",
        "    - **`fine_tuned_models`**: Unique model identifiers are extracted to iterate over each model’s predictions.\n",
        "    - For each model, the following steps are performed to calculate portfolio returns and metrics.\n",
        "\n",
        "2. **Processing Each Fine-Tuned Model**:\n",
        "    - **`process_model()`**: This function is used to calculate portfolio returns for both equally weighted (EW) and value weighted (VW) portfolios:\n",
        "        - **EW and VW Long**: Returns for the top N stocks.\n",
        "        - **EW and VW Short**: Returns for the bottom N stocks.\n",
        "        - **EW and VW Long-Short (LS)**: Returns for a portfolio that goes long on the top N stocks and short on the bottom N stocks.\n",
        "    - **Transaction Costs**: Portfolio returns with and without transaction costs are considered, though in this case, the adjusted returns are commented out.\n",
        "\n",
        "3. **Calculating and Storing Sharpe Ratios**:\n",
        "    - **`calculate_sharpe_ratio()`**: Sharpe ratios are calculated for each portfolio configuration (EW L, EW S, EW LS, VW L, VW S, VW LS) as well as for the market benchmark.\n",
        "    - The results are stored in a list of dictionaries, where each dictionary contains the Sharpe ratios for a specific model.\n",
        "\n",
        "4. **Visualizing Cumulative Returns**:\n",
        "    - **Cumulative Returns Plot**: For each model, cumulative returns are plotted over time to visualize the performance of the portfolios.\n",
        "        - The cumulative log returns for each portfolio and the market benchmark are plotted, with different line styles distinguishing between unadjusted and adjusted returns.\n",
        "\n",
        "5. **Saving the Sharpe Ratios**:\n",
        "    - **Conversion to DataFrame**: The list of Sharpe ratios is converted into a DataFrame for easier analysis and saving.\n",
        "    - **Saving to CSV**: The resulting DataFrame containing Sharpe ratios for all fine-tuned models is saved to a CSV file for further analysis."
      ],
      "metadata": {
        "id": "9tyBj9NK3kVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 14: Process each model's fine-tuned predictions\n",
        "fine_tuned_predicted_directions = pd.read_csv('/content/merged_eval_df.csv')\n",
        "fine_tuned_models = fine_tuned_predicted_directions['Model'].unique()\n",
        "fine_tuned_all_sharpe_ratios = []\n",
        "\n",
        "for model in fine_tuned_models:\n",
        "    ew_l, ew_s, ew_ls, vw_l, vw_s, vw_ls, portfolio_returns, market_returns = process_model(\n",
        "        model, fine_tuned_predicted_directions, returns_data_pivot, market_returns)\n",
        "    adj_ew_l, adj_ew_s, adj_ew_ls, adj_vw_l, adj_vw_s, adj_vw_ls, adj_portfolio_returns, adj_market_returns = process_model(\n",
        "        model, fine_tuned_predicted_directions, adjusted_returns_data_pivot, adjusted_market_returns)\n",
        "\n",
        "    if ew_l is None or vw_l is None:\n",
        "        continue\n",
        "\n",
        "    model_sharpe_ratios = {\n",
        "        'Model': model,\n",
        "        'EW L': calculate_sharpe_ratio(ew_l),\n",
        "        'EW S': calculate_sharpe_ratio(ew_s),\n",
        "        'EW LS': calculate_sharpe_ratio(ew_ls),\n",
        "        'VW L': calculate_sharpe_ratio(vw_l),\n",
        "        'VW S': calculate_sharpe_ratio(vw_s),\n",
        "        'VW LS': calculate_sharpe_ratio(vw_ls),\n",
        "        'Market': calculate_sharpe_ratio(market_returns),\n",
        "        # 'Adj EW L': calculate_sharpe_ratio(adj_ew_l),\n",
        "        # 'Adj EW S': calculate_sharpe_ratio(adj_ew_s),\n",
        "        # 'Adj EW LS': calculate_sharpe_ratio(adj_ew_ls),\n",
        "        # 'Adj VW L': calculate_sharpe_ratio(adj_vw_l),\n",
        "        # 'Adj VW S': calculate_sharpe_ratio(adj_vw_s),\n",
        "        # 'Adj VW LS': calculate_sharpe_ratio(adj_vw_ls),\n",
        "        # 'Adj Market': calculate_sharpe_ratio(adj_market_returns)\n",
        "    }\n",
        "\n",
        "    fine_tuned_all_sharpe_ratios.append(model_sharpe_ratios)\n",
        "\n",
        "    # Plotting Cumulative Returns\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for label, returns in portfolio_returns.items():\n",
        "        plt.plot((1 + returns).cumprod() - 1, label=label)\n",
        "    # for label, returns in adj_portfolio_returns.items():\n",
        "    #     plt.plot((1 + returns).cumprod() - 1, label=label, linestyle='--')\n",
        "    plt.plot((1 + market_returns).cumprod() - 1, label='Market', linestyle=':')\n",
        "    # plt.plot((1 + adj_market_returns).cumprod() - 1, label='Adj Market', linestyle='-.')\n",
        "    plt.title(f'Cumulative Weekly Portfolio Returns Over Time for Fine-Tuned {model}')\n",
        "    plt.xlabel('Date', fontsize=14)\n",
        "    plt.ylabel('Cumulative Log Returns', fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Step 15: Convert Sharpe ratios to DataFrame and save\n",
        "fine_tuned_sharpe_ratios_df = pd.DataFrame(fine_tuned_all_sharpe_ratios)\n",
        "print(fine_tuned_sharpe_ratios_df)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "fine_tuned_sharpe_ratios_df.to_csv('/content/fine_tuned_sharpe_ratios.csv', index=False)"
      ],
      "metadata": {
        "id": "LDS3mzc33Cxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing and Analyzing Un-Tuned Model Predictions for Risk and Return Performance\n",
        "\n",
        "This section focuses on processing predictions from various models to calculate risk and return statistics, ultimately creating a detailed performance analysis.\n",
        "\n",
        "### Key Steps in the Process\n",
        "\n",
        "1. **Loading Predictions**:\n",
        "    - **`un_tuned_predicted_directions`**: The DataFrame containing the predicted directions for each stock based on un-tuned models is loaded from a CSV file.\n",
        "    - **`un_tuned_models`**: Unique model identifiers are extracted to iterate over each model’s predictions.\n",
        "\n",
        "2. **Processing Each Model's Predictions**:\n",
        "    - For each model, predictions are processed under two scenarios:\n",
        "        1. **Without Transaction Costs**: Raw portfolio returns are calculated without accounting for transaction costs.\n",
        "        2. **With Transaction Costs**: Portfolio returns are adjusted by applying transaction costs.\n",
        "    - **`process_model()`**: This function calculates portfolio returns for both equally weighted (EW) and value weighted (VW) portfolios, including long-only, short-only, and long-short configurations.\n",
        "\n",
        "3. **Calculating and Storing Sharpe Ratios**:\n",
        "    - **`calculate_sharpe_ratio()`**: Sharpe ratios are calculated for each portfolio configuration (EW L, EW S, EW LS, VW L, VW S, VW LS).\n",
        "    - **Performance Metrics**: For each portfolio type and scenario, the following metrics are stored:\n",
        "        - **Return**: The mean return of the portfolio.\n",
        "        - **Standard Deviation (Std)**: The volatility of the portfolio returns.\n",
        "        - **Sharpe Ratio (SR)**: A measure of risk-adjusted return.\n",
        "\n",
        "4. **Creating a Performance Summary**:\n",
        "    - **`pivot_df`**: The collected results are pivoted into a multi-level DataFrame for better presentation, organizing metrics by model, scenario, and portfolio type.\n",
        "    - **Styling and Saving**:\n",
        "        - **Pandas Styling**: The DataFrame is styled for readability, with numeric formatting applied to the performance metrics.\n",
        "        - **Saving Outputs**: The styled DataFrame is saved as an HTML file, and the underlying data is also saved as an Excel file.\n"
      ],
      "metadata": {
        "id": "G9p0bDSH4pu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 16: Process each model's predictions and calculate statistics\n",
        "un_tuned_predicted_directions = pd.read_csv('/content/predicted_rolling_window_test_directions.csv')\n",
        "un_tuned_models = un_tuned_predicted_directions['Model'].unique()\n",
        "un_tuned_all_sharpe_ratios = []\n",
        "\n",
        "results = {\n",
        "    'Model': [],\n",
        "    'Scenario': [],\n",
        "    'Type': [],\n",
        "    'Return': [],\n",
        "    'Std': [],\n",
        "    'SR': []\n",
        "}\n",
        "\n",
        "for model in un_tuned_models:\n",
        "    for scenario, data, market_ret in [\n",
        "        ('Without Transaction Cost', returns_data_pivot, market_returns),\n",
        "        ('With Transaction Cost', adjusted_returns_data_pivot, adjusted_market_returns)\n",
        "    ]:\n",
        "        ew_l, ew_s, ew_ls, vw_l, vw_s, vw_ls, _, _ = process_model(\n",
        "            model, un_tuned_predicted_directions, data, market_ret)\n",
        "\n",
        "        model_sharpe_ratios = {\n",
        "            'EW L': calculate_sharpe_ratio(ew_l),\n",
        "            'EW S': calculate_sharpe_ratio(ew_s),\n",
        "            'EW LS': calculate_sharpe_ratio(ew_ls),\n",
        "            'VW L': calculate_sharpe_ratio(vw_l),\n",
        "            'VW S': calculate_sharpe_ratio(vw_s),\n",
        "            'VW LS': calculate_sharpe_ratio(vw_ls)\n",
        "        }\n",
        "\n",
        "        for port_type, returns_data in [\n",
        "            ('EW L', ew_l),\n",
        "            ('EW S', ew_s),\n",
        "            ('EW LS', ew_ls),\n",
        "            ('VW L', vw_l),\n",
        "            ('VW S', vw_s),\n",
        "            ('VW LS', vw_ls)\n",
        "        ]:\n",
        "            results['Model'].append(model)\n",
        "            results['Scenario'].append(scenario)\n",
        "            results['Type'].append(port_type)\n",
        "            results['Return'].append(returns_data.mean())\n",
        "            results['Std'].append(returns_data.std())\n",
        "            results['SR'].append(model_sharpe_ratios[port_type])\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Pivot the DataFrame to get a multi-level index for better presentation\n",
        "pivot_df = results_df.pivot_table(\n",
        "    index=['Type', 'Scenario'],\n",
        "    columns='Model',\n",
        "    values=['Return', 'Std', 'SR']\n",
        ")\n",
        "\n",
        "# Reorder for the presentation similar to the image\n",
        "pivot_df = pivot_df.swaplevel(0, 1).sort_index(level=0)\n",
        "pivot_df.columns = [' '.join(col).strip() for col in pivot_df.columns.values]\n",
        "pivot_df = pivot_df.reset_index()\n",
        "\n",
        "\n",
        "# Display the DataFrame in a readable format with Pandas styling\n",
        "styled_df = pivot_df.style.format({\n",
        "    'Return': '{:.3f}',\n",
        "    'Std': '{:.3f}',\n",
        "    'SR': '{:.3f}'\n",
        "}).set_caption(\"Risk and Return Performance for Pre-trained LLMs\")\n",
        "\n",
        "styled_df = styled_df.set_table_styles({\n",
        "    'Scenario': [\n",
        "        {'selector': 'th', 'props': [('font-size', '10pt'), ('text-align', 'center')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]},\n",
        "    ],\n",
        "    'Model': [\n",
        "        {'selector': 'th', 'props': [('font-size', '10pt'), ('text-align', 'center')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]},\n",
        "    ],\n",
        "    'Type': [\n",
        "        {'selector': 'th', 'props': [('font-size', '10pt'), ('text-align', 'center')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]},\n",
        "    ]\n",
        "})\n",
        "\n",
        "styled_df.to_html('/content/risk_and_return_performance_pre_trained.html')\n",
        "pivot_df.to_excel('/content/risk_and_return_performance_pre_trained.xlsx', index=False)"
      ],
      "metadata": {
        "id": "IBUnnW3K3DjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing and Analyzing Fine-Tuned Model Predictions for Risk and Return Performance\n",
        "\n",
        "This section is focused on processing predictions from fine-tuned models to calculate risk and return statistics, providing a detailed performance analysis.\n",
        "\n",
        "### Key Steps in the Process\n",
        "\n",
        "1. **Loading Predictions**:\n",
        "    - **`fine_tuned_predicted_directions`**: The DataFrame containing the predicted directions for each stock based on fine-tuned models is loaded from a CSV file.\n",
        "    - **`fine_tuned_predicted_models`**: Unique model identifiers are extracted to iterate over each model’s predictions.\n",
        "\n",
        "2. **Processing Each Model's Predictions**:\n",
        "    - For each model, predictions are processed under two scenarios:\n",
        "        1. **Without Transaction Costs**: Raw portfolio returns are calculated without accounting for transaction costs.\n",
        "        2. **With Transaction Costs**: Portfolio returns are adjusted by applying transaction costs.\n",
        "    - **`process_model()`**: This function calculates portfolio returns for both equally weighted (EW) and value weighted (VW) portfolios, including long-only, short-only, and long-short configurations.\n",
        "\n",
        "3. **Calculating and Storing Sharpe Ratios**:\n",
        "    - **`calculate_sharpe_ratio()`**: Sharpe ratios are calculated for each portfolio configuration (EW L, EW S, EW LS, VW L, VW S, VW LS).\n",
        "    - **Performance Metrics**: For each portfolio type and scenario, the following metrics are stored:\n",
        "        - **Return**: The mean return of the portfolio.\n",
        "        - **Standard Deviation (Std)**: The volatility of the portfolio returns.\n",
        "        - **Sharpe Ratio (SR)**: A measure of risk-adjusted return.\n",
        "\n",
        "4. **Creating a Performance Summary**:\n",
        "    - **`pivot_df`**: The collected results are pivoted into a multi-level DataFrame for better presentation, organizing metrics by model, scenario, and portfolio type.\n",
        "    - **Styling and Saving**:\n",
        "        - **Pandas Styling**: The DataFrame is styled for readability, with numeric formatting applied to the performance metrics.\n",
        "        - **Saving Outputs**: The styled DataFrame is saved as an HTML file, and the underlying data is also saved as an Excel file."
      ],
      "metadata": {
        "id": "2N7Yn9BF5Opy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 17: Process each model's predictions and calculate statistics\n",
        "fine_tuned_predicted_directions = pd.read_csv('/content/merged_fine_tuned_results_df.csv')\n",
        "fine_tuned_predicted_models = fine_tuned_predicted_directions['Model'].unique()\n",
        "un_tuned_all_sharpe_ratios = []\n",
        "\n",
        "results = {\n",
        "    'Model': [],\n",
        "    'Scenario': [],\n",
        "    'Type': [],\n",
        "    'Return': [],\n",
        "    'Std': [],\n",
        "    'SR': []\n",
        "}\n",
        "\n",
        "for model in fine_tuned_predicted_models:\n",
        "    for scenario, data, market_ret in [\n",
        "        ('Without Transaction Cost', returns_data_pivot, market_returns),\n",
        "        ('With Transaction Cost', adjusted_returns_data_pivot, adjusted_market_returns)\n",
        "    ]:\n",
        "        ew_l, ew_s, ew_ls, vw_l, vw_s, vw_ls, _, _ = process_model(\n",
        "            model, fine_tuned_predicted_directions, data, market_ret)\n",
        "\n",
        "        model_sharpe_ratios = {\n",
        "            'EW L': calculate_sharpe_ratio(ew_l),\n",
        "            'EW S': calculate_sharpe_ratio(ew_s),\n",
        "            'EW LS': calculate_sharpe_ratio(ew_ls),\n",
        "            'VW L': calculate_sharpe_ratio(vw_l),\n",
        "            'VW S': calculate_sharpe_ratio(vw_s),\n",
        "            'VW LS': calculate_sharpe_ratio(vw_ls)\n",
        "        }\n",
        "\n",
        "        for port_type, returns_data in [\n",
        "            ('EW L', ew_l),\n",
        "            ('EW S', ew_s),\n",
        "            ('EW LS', ew_ls),\n",
        "            ('VW L', vw_l),\n",
        "            ('VW S', vw_s),\n",
        "            ('VW LS', vw_ls)\n",
        "        ]:\n",
        "            results['Model'].append(model)\n",
        "            results['Scenario'].append(scenario)\n",
        "            results['Type'].append(port_type)\n",
        "            results['Return'].append(returns_data.mean())\n",
        "            results['Std'].append(returns_data.std())\n",
        "            results['SR'].append(model_sharpe_ratios[port_type])\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Pivot the DataFrame to get a multi-level index for better presentation\n",
        "pivot_df = results_df.pivot_table(\n",
        "    index=['Type', 'Scenario'],\n",
        "    columns='Model',\n",
        "    values=['Return', 'Std', 'SR']\n",
        ")\n",
        "\n",
        "# Reorder for the presentation similar to the image\n",
        "pivot_df = pivot_df.swaplevel(0, 1).sort_index(level=0)\n",
        "pivot_df.columns = [' '.join(col).strip() for col in pivot_df.columns.values]\n",
        "pivot_df = pivot_df.reset_index()\n",
        "\n",
        "\n",
        "# Display the DataFrame in a readable format with Pandas styling\n",
        "styled_df = pivot_df.style.format({\n",
        "    'Return': '{:.3f}',\n",
        "    'Std': '{:.3f}',\n",
        "    'SR': '{:.3f}'\n",
        "}).set_caption(\"Risk and Return Performance for Fine-trained LLMs\")\n",
        "\n",
        "styled_df = styled_df.set_table_styles({\n",
        "    'Scenario': [\n",
        "        {'selector': 'th', 'props': [('font-size', '10pt'), ('text-align', 'center')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]},\n",
        "    ],\n",
        "    'Model': [\n",
        "        {'selector': 'th', 'props': [('font-size', '10pt'), ('text-align', 'center')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]},\n",
        "    ],\n",
        "    'Type': [\n",
        "        {'selector': 'th', 'props': [('font-size', '10pt'), ('text-align', 'center')]},\n",
        "        {'selector': 'td', 'props': [('text-align', 'center')]},\n",
        "    ]\n",
        "})\n",
        "\n",
        "styled_df.to_html('/content/risk_and_return_performance_fine_tuned.html')\n",
        "pivot_df.to_excel('/content/risk_and_return_performance_fine_tuned.xlsx', index=False)"
      ],
      "metadata": {
        "id": "G-R-kj1R3DTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End Notes\n",
        "\n",
        "This analysis highlights the effectiveness of both pre-trained and fine-tuned language models in predicting market movements and constructing portfolios. By calculating and comparing key metrics such as returns, volatility, and Sharpe ratios across various scenarios (with and without transaction costs), we have gained a deeper understanding of each model's performance.\n",
        "\n",
        "The results demonstrate that while pre-trained models offer a solid baseline, fine-tuned models generally yield superior risk-adjusted returns, particularly in more nuanced market conditions. This reinforces the value of domain-specific fine-tuning in enhancing predictive accuracy and optimizing portfolio performance."
      ],
      "metadata": {
        "id": "2plZsDnW6SIS"
      }
    }
  ]
}